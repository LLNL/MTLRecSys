{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STL and MTL GP Regression Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "#from design import ModelTraining\n",
    "from datasets import SyntheticData as SD\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import methods.mtl.MTL_GP as MtlGP\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Datasets\n",
    "\n",
    "The very first step to running through these Gaussian Process Tutorials is retrieving some data to train our models on. Here we are using the CTRP, GDSC and CCLE datasets mentioned in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(MtlGP)\n",
    "dataset = SD.SyntheticDataCreator(num_tasks=3,cellsPerTask=400, drugsPerTask=10, function=\"cosine\",\n",
    "             normalize=False, noise=1, graph=False, test_split=0.3)\n",
    "dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Task Gaussian Process Example\n",
    "\n",
    "below is an exaple of training and testing a basic Sparse Gaussian Process from gpytorch with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import methods.regressor.SparseGP as SGP\n",
    "importlib.reload(SGP)\n",
    "\n",
    "y_pred = {}\n",
    "sparsegp = SGP.SparseGPRegression(num_iters=50, length_scale=50, noise_covar=1.5, n_inducing_points=250)\n",
    "for k in dataset.datasets:\n",
    "    sparsegp.fit(dataset.data['train']['x'][k],\n",
    "               y=dataset.data['train']['y'][k],\n",
    "               cat_point=dataset.cat_point)\n",
    "    y_pred[k] = sparsegp.predict(dataset.data['test']['x'][k])\n",
    "    \n",
    "for name in y_pred.keys():\n",
    "    rmse = np.sqrt(np.sum(((y_pred[name] - dataset.data['test']['y'][name]) ** 2) / len(y_pred[name])))\n",
    "    print(rmse, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have a more complex method, composite kernel Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import methods.regressor.SparseGPCompositeKernel as sgpc\n",
    "importlib.reload(sgpc)\n",
    "y_pred = {}\n",
    "sparsegpcomp = sgpc.SparseGPCompositeKernelRegression(num_iters=10, length_scale_cell=100, length_scale_drug=100, noise_covar=1.5, n_inducing_points=500, learning_rate=.1)\n",
    "for k in dataset.datasets:\n",
    "    sparsegpcomp.fit(dataset.data['train']['x'][k],\n",
    "               y=dataset.data['train']['y'][k],\n",
    "               cat_point=dataset.cat_point)\n",
    "    y_pred[k] = sparsegpcomp.predict(dataset.data['test']['x'][k])\n",
    "    \n",
    "for name in y_pred.keys():\n",
    "    rmse = np.sqrt(np.sum(((y_pred[name] - dataset.data['test']['y'][name]) ** 2) / len(y_pred[name])))\n",
    "    print(rmse, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask Background\n",
    "\n",
    "Given a set of observations $y_0$ we wish to learn parameters $\\theta_x$ and $k^x$ of the matrix $K_f$. $k^x$ is a covariance function over the inputs and $\\theta_x$ are the parameters for that specific covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadamard Product MTL \n",
    "A clear limitation of the last method is that although it is technically multitask, it will fail to capture most task relationships. In order to do this I'll introduce another spin on vanilla GP Regression.\n",
    "\n",
    "Now we just have one model parameterized as:\n",
    "\\begin{align*} \n",
    "y_{i} &= f(x_i) + \\varepsilon_{i} \\\\\n",
    "f &\\sim \\mathcal{GP}(C_t,K_{\\theta}) \\\\ \n",
    "\\theta &\\sim p(\\theta) \\\\ \n",
    "\\varepsilon_{i} &\\stackrel{iid}{\\sim} \\mathcal{N}(0, \\sigma^2)  \\ \n",
    "\\end{align*}\n",
    "\n",
    "With one key difference. Our kernel is now defined as: $K([x,i],[x',j]) = k_{inputs}(x,x') * k_{tasks}(i,j)$ where $ k_{tasks} $ is an \"index kernel\", essentially a lookup table for inter-task covariance. This lookup table is defined $\\forall \\ i,j \\in$ the set of tasks $T$. Here's a basic example with 4 datapoints and 2 tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(MtlGP)\n",
    "\n",
    "hadamardMTL = MtlGP.HadamardMTL(num_iters=300, length_scale=20, noise_covar=.24, n_inducing_points=500, \\\n",
    "                                composite=False, learning_rate=.07, validate=False,bias=False,stabilize=False)\n",
    "\n",
    "\n",
    "hadamardMTL.fit(dataset.data['train']['x'],\n",
    "                                   y=dataset.data['train']['y'],\n",
    "                                   catpt=dataset.cat_point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = hadamardMTL.predict(dataset.data['test']['x']) \n",
    "for name in y_pred.keys():\n",
    "    rmse = np.sqrt(np.sum(((y_pred[name].numpy() - dataset.data['test']['y'][name]) ** 2) / len(y_pred[name])))\n",
    "    print(rmse, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Visualizing Covariance Using Getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_covar = hadamardMTL.model.getCovar().numpy()\n",
    "plt.imshow(full_covar)\n",
    "plt.imshow(hadamardMTL.model.getCovar().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig, ax = plt.subplots()\n",
    "task_covar = hadamardMTL.model.getTaskCovar().numpy() # cast from torch to numpy\n",
    "im = ax.imshow(task_covar, cmap=\"Reds\")\n",
    "ax.set_xticks([200,800,1300])\n",
    "ax.set_xticklabels(dataset.datasets)\n",
    "ax.set_yticks([200,800,1300])\n",
    "ax.set_yticklabels(dataset.datasets)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"10%\", pad=0.5)\n",
    "cbar = plt.colorbar(im, cax = cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Multitask GP with Multitask Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(MtlGP)\n",
    "\n",
    "gpymtl = MtlGP.GPyFullMTL(num_iters=300, length_scale=15, noise_covar=1, n_inducing_points=200,  num_tasks=3, learning_rate=.05)\n",
    "\n",
    "\n",
    "gpymtl.fit(dataset.data['train']['x'],\n",
    "                                   y=dataset.data['train']['y'],\n",
    "                                   cat_point=dataset.cat_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5526002760370554 0\n",
      "0.721262580126851 1\n",
      "0.7105683397091712 2\n"
     ]
    }
   ],
   "source": [
    "y_pred = gpymtl.predict(dataset.data['test']['x']) \n",
    "i = 0\n",
    "for name in y_pred.keys():\n",
    "    rmse = np.sqrt(np.sum(((y_pred[name] - dataset.data['test']['y'][name]) ** 2) / len(y_pred[name])))\n",
    "    i +=  1\n",
    "    print(rmse, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Find Initial Conditions\n",
    "\n",
    "In order to understand what parameters to start at, we can test different configurations of initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(MtlGP)\n",
    "multiBias = MtlGP.HadamardMTL(num_iters=10, noise_covar=1.5, n_inducing_points=500, multitask_kernel=False)   #testing #0)\n",
    "\n",
    "multiBias._find_initial_conditions(dataset.data['train']['x'], dataset.data['train']['y'], \\\n",
    "                                   n_restarts=800,n_iters=50, n_inducing_points=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tensor(1.2674, grad_fn=<NegBackward>),\n",
    " {'likelihood.noise_covar.noise': 0.7006388902664185,\n",
    "  'covar_module.lengthscale': 10.444199562072754})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
