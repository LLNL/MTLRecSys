%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english,openany,oneside]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{DejaVu Serif}
\setsansfont{DejaVu Sans}
\setmonofont{DejaVu Sans Mono}


\usepackage[Bjornstrup]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}


% Jupyter Notebook code cell colors
\definecolor{nbsphinxin}{HTML}{307FC1}
\definecolor{nbsphinxout}{HTML}{BF5B3D}
\definecolor{nbsphinx-code-bg}{HTML}{F5F5F5}
\definecolor{nbsphinx-code-border}{HTML}{E0E0E0}
\definecolor{nbsphinx-stderr}{HTML}{FFDDDD}
% ANSI colors for output streams and traceback highlighting
\definecolor{ansi-black}{HTML}{3E424D}
\definecolor{ansi-black-intense}{HTML}{282C36}
\definecolor{ansi-red}{HTML}{E75C58}
\definecolor{ansi-red-intense}{HTML}{B22B31}
\definecolor{ansi-green}{HTML}{00A250}
\definecolor{ansi-green-intense}{HTML}{007427}
\definecolor{ansi-yellow}{HTML}{DDB62B}
\definecolor{ansi-yellow-intense}{HTML}{B27D12}
\definecolor{ansi-blue}{HTML}{208FFB}
\definecolor{ansi-blue-intense}{HTML}{0065CA}
\definecolor{ansi-magenta}{HTML}{D160C4}
\definecolor{ansi-magenta-intense}{HTML}{A03196}
\definecolor{ansi-cyan}{HTML}{60C6C8}
\definecolor{ansi-cyan-intense}{HTML}{258F8F}
\definecolor{ansi-white}{HTML}{C5C1B4}
\definecolor{ansi-white-intense}{HTML}{A1A6B2}
\definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
\definecolor{ansi-default-inverse-bg}{HTML}{000000}

% Define an environment for non-plain-text code cell outputs (e.g. images)
\makeatletter
\newenvironment{nbsphinxfancyoutput}{%
    % Avoid fatal error with framed.sty if graphics too long to fit on one page
    \let\sphinxincludegraphics\nbsphinxincludegraphics
    \nbsphinx@image@maxheight\textheight
    \advance\nbsphinx@image@maxheight -2\fboxsep   % default \fboxsep 3pt
    \advance\nbsphinx@image@maxheight -2\fboxrule  % default \fboxrule 0.4pt
    \advance\nbsphinx@image@maxheight -\baselineskip
\def\nbsphinxfcolorbox{\spx@fcolorbox{nbsphinx-code-border}{white}}%
\def\FrameCommand{\nbsphinxfcolorbox\nbsphinxfancyaddprompt\@empty}%
\def\FirstFrameCommand{\nbsphinxfcolorbox\nbsphinxfancyaddprompt\sphinxVerbatim@Continues}%
\def\MidFrameCommand{\nbsphinxfcolorbox\sphinxVerbatim@Continued\sphinxVerbatim@Continues}%
\def\LastFrameCommand{\nbsphinxfcolorbox\sphinxVerbatim@Continued\@empty}%
\MakeFramed{\advance\hsize-\width\@totalleftmargin\z@\linewidth\hsize\@setminipage}%
\lineskip=1ex\lineskiplimit=1ex\raggedright%
}{\par\unskip\@minipagefalse\endMakeFramed}
\makeatother
\newbox\nbsphinxpromptbox
\def\nbsphinxfancyaddprompt{\ifvoid\nbsphinxpromptbox\else
    \kern\fboxrule\kern\fboxsep
    \copy\nbsphinxpromptbox
    \kern-\ht\nbsphinxpromptbox\kern-\dp\nbsphinxpromptbox
    \kern-\fboxsep\kern-\fboxrule\nointerlineskip
    \fi}
\newlength\nbsphinxcodecellspacing
\setlength{\nbsphinxcodecellspacing}{0pt}

% Define support macros for attaching opening and closing lines to notebooks
\newsavebox\nbsphinxbox
\makeatletter
\newcommand{\nbsphinxstartnotebook}[1]{%
    \par
    % measure needed space
    \setbox\nbsphinxbox\vtop{{#1\par}}
    % reserve some space at bottom of page, else start new page
    \needspace{\dimexpr2.5\baselineskip+\ht\nbsphinxbox+\dp\nbsphinxbox}
    % mimick vertical spacing from \section command
      \addpenalty\@secpenalty
      \@tempskipa 3.5ex \@plus 1ex \@minus .2ex\relax
      \addvspace\@tempskipa
      {\Large\@tempskipa\baselineskip
             \advance\@tempskipa-\prevdepth
             \advance\@tempskipa-\ht\nbsphinxbox
             \ifdim\@tempskipa>\z@
               \vskip \@tempskipa
             \fi}
    \unvbox\nbsphinxbox
    % if notebook starts with a \section, prevent it from adding extra space
    \@nobreaktrue\everypar{\@nobreakfalse\everypar{}}%
    % compensate the parskip which will get inserted by next paragraph
    \nobreak\vskip-\parskip
    % do not break here
    \nobreak
}% end of \nbsphinxstartnotebook

\newcommand{\nbsphinxstopnotebook}[1]{%
    \par
    % measure needed space
    \setbox\nbsphinxbox\vbox{{#1\par}}
    \nobreak % it updates page totals
    \dimen@\pagegoal
    \advance\dimen@-\pagetotal \advance\dimen@-\pagedepth
    \advance\dimen@-\ht\nbsphinxbox \advance\dimen@-\dp\nbsphinxbox
    \ifdim\dimen@<\z@
      % little space left
      \unvbox\nbsphinxbox
      \kern-.8\baselineskip
      \nobreak\vskip\z@\@plus1fil
      \penalty100
      \vskip\z@\@plus-1fil
      \kern.8\baselineskip
    \else
      \unvbox\nbsphinxbox
    \fi
}% end of \nbsphinxstopnotebook

% Ensure height of an included graphics fits in nbsphinxfancyoutput frame
\newdimen\nbsphinx@image@maxheight % set in nbsphinxfancyoutput environment
\newcommand*{\nbsphinxincludegraphics}[2][]{%
    \gdef\spx@includegraphics@options{#1}%
    \setbox\spx@image@box\hbox{\includegraphics[#1,draft]{#2}}%
    \in@false
    \ifdim \wd\spx@image@box>\linewidth
      \g@addto@macro\spx@includegraphics@options{,width=\linewidth}%
      \in@true
    \fi
    % no rotation, no need to worry about depth
    \ifdim \ht\spx@image@box>\nbsphinx@image@maxheight
      \g@addto@macro\spx@includegraphics@options{,height=\nbsphinx@image@maxheight}%
      \in@true
    \fi
    \ifin@
      \g@addto@macro\spx@includegraphics@options{,keepaspectratio}%
    \fi
    \setbox\spx@image@box\box\voidb@x % clear memory
    \expandafter\includegraphics\expandafter[\spx@includegraphics@options]{#2}%
}% end of "\MakeFrame"-safe variant of \sphinxincludegraphics
\makeatother

\makeatletter
\renewcommand*\sphinx@verbatim@nolig@list{\do\'\do\`}
\begingroup
\catcode`'=\active
\let\nbsphinx@noligs\@noligs
\g@addto@macro\nbsphinx@noligs{\let'\PYGZsq}
\endgroup
\makeatother
\renewcommand*\sphinxbreaksbeforeactivelist{\do\<\do\"\do\'}
\renewcommand*\sphinxbreaksafteractivelist{\do\.\do\,\do\:\do\;\do\?\do\!\do\/\do\>\do\-}
\makeatletter
\fvset{codes*=\sphinxbreaksattexescapedchars\do\^\^\let\@noligs\nbsphinx@noligs}
\makeatother


\usepackage[titles]{tocloft}
\cftsetpnumwidth {1.25cm}\cftsetrmarg{1.5cm}
\setlength{\cftchapnumwidth}{0.75cm}
\setlength{\cftsecindent}{\cftchapnumwidth}
\setlength{\cftsecnumwidth}{1.25cm}


\title{Multitask Recommender Systems for Cancer Drug Response}
\date{Sep 26, 2020}
\release{9/23/2020}
\author{}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


Hello, this is intended to be technical documentation that will make it easier to use data pipeline and the methods for recommender systems
in this project. So far, this proejct consists of wrappers around open source software purposed for multitask learning between datasets containing drug/cancer interactions. For now, these docs aren’t intended for developers trying to use this as an API but just collaborators who might want to use the code directly.

If you are reading this and have any suggestions for features to be added / changes to be made email me at \sphinxhref{mailto:ladd12@llnl.gov}{ladd12@llnl.gov}


\chapter{Introduction}
\label{\detokenize{intro:introduction}}\label{\detokenize{intro::doc}}

\section{Background}
\label{\detokenize{intro:background}}
The goal of this code is to accurately model the interactions between cancer drugs and cancer cell lines across multiple datasets. Datasets can be from:
\begin{itemize}
\item {} 
\sphinxhref{https://www.cancer.gov/}{National Cancer Institute (NCI)}%
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.cancer.gov/}
%
\end{footnote} \sphinxcite{zcite:nci}

\item {} 
\sphinxhref{https://portals.broadinstitute.org/ccle}{Broad Institute Cancer and Cell Line Encyclopedia}%
\begin{footnote}[2]\sphinxAtStartFootnote
\sphinxnolinkurl{https://portals.broadinstitute.org/ccle}
%
\end{footnote} \sphinxcite{zcite:ccle}

\item {} 
\sphinxhref{https://www.cancerrxgene.org/}{Genomics and Drug Sensitivity in Cancer}%
\begin{footnote}[3]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.cancerrxgene.org/}
%
\end{footnote}

\item {} 
\sphinxhref{https://www.cancer.gov/about-nci/organization/ccct/ctrp}{National Institute of Health Clincial Trials Reporting Program}%
\begin{footnote}[4]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.cancer.gov/about-nci/organization/ccct/ctrp}
%
\end{footnote}\sphinxcite{zcite:nci}

\end{itemize}


\section{Problem Statement}
\label{\detokenize{intro:problem-statement}}
We need to have each of the following things from each dataset: a sparse ratings/interaction matrix indicating the (IC50) effective concentrations of each drug for each type of cancer and features for each respective drug and cell line. Here are some examples of services that generate theses features:
\begin{itemize}
\item {} 
\sphinxhref{https://chm.kode-solutions.net/products\_dragon.php}{Dragon7 (discontinued)}%
\begin{footnote}[5]\sphinxAtStartFootnote
\sphinxnolinkurl{https://chm.kode-solutions.net/products\_dragon.php}
%
\end{footnote}

\item {} 
\sphinxhref{https://www.chemcomp.com/Products.htm}{MOE}%
\begin{footnote}[6]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.chemcomp.com/Products.htm}
%
\end{footnote}

\item {} 
\sphinxhref{http://www.lincsproject.org/}{Lincs Cell Features}%
\begin{footnote}[7]\sphinxAtStartFootnote
\sphinxnolinkurl{http://www.lincsproject.org/}
%
\end{footnote}

\end{itemize}

Then we try to fit machine learning models to this data \sphinxstylestrong{to create valid predictors what the outcome of future drug and cancer line interactions will be} in order to inform future experiments and clinical doctors. Finally, in this introduction it will be useful to outline what ML methods we are using and some basic properties.


\section{Methods}
\label{\detokenize{intro:methods}}

\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{ML Methods Used}\label{\detokenize{intro:id10}}
\sphinxaftertopcaption
\begin{tabular}[t]{|\X{50}{125}|\X{25}{125}|\X{25}{125}|\X{25}{125}|}
\hline
\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
MTL or STL
&\sphinxstyletheadfamily 
Feature Based?
&\sphinxstyletheadfamily 
Source
\\
\hline
Collaborative Filtering Matrix Factorization
&
STL and MTL
&
Feature Based
&
Surprise \sphinxcite{zcite:hug2020}
\\
\hline
K Nearest Neighbors
&
STL
&
Not Feature Based
&
Surprise \sphinxcite{zcite:hug2020}
\\
\hline
Nonnegative Matrix Factorization
&
STL
&
Feature Based
&
Surprise \sphinxcite{zcite:hug2020}
\\
\hline
Feedforward Neural Net
&
STL
&
Feature Based
&
Custom w/ Pytorch \sphinxcite{zcite:neurips2019-9015}
\\
\hline
Gaussian Process
&
STL and MTL
&
Feature Based
&
Gpytorch \sphinxcite{zcite:gardner2018gpytorch}
\\
\hline
Neural Collaborative Filtering
&
STL and MTL
&
Both
&
Author Github \sphinxcite{zcite:ncf}
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}


\chapter{Installation}
\label{\detokenize{install:installation}}\label{\detokenize{install::doc}}

\section{Using Conda}
\label{\detokenize{install:using-conda}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Navigate to root directory, where environment.yml is located

\item {} 
conda env create \sphinxhyphen{}f environment.yml

\item {} 
conda activate mtl4c\_env

\item {} 
Verify that environment installed correctly with conda env list

\end{enumerate}

Open to any other suggestions for env sharing, like docker.


\chapter{Data}
\label{\detokenize{data:data}}\label{\detokenize{data::doc}}

\section{Synthetic Data}
\label{\detokenize{data:synthetic-data}}
We can’t publish real data, so instead there is a file that will load synthetic data. This file and it’s attributes are important for adapting this software. If you want to use this code, the best way would be to create a loader class with similar methods to the methods outlined below.

\phantomsection\label{\detokenize{data:module-datasets.SyntheticData}}\index{module@\spxentry{module}!datasets.SyntheticData@\spxentry{datasets.SyntheticData}}\index{datasets.SyntheticData@\spxentry{datasets.SyntheticData}!module@\spxentry{module}}\index{SyntheticDataCreator (class in datasets.SyntheticData)@\spxentry{SyntheticDataCreator}\spxextra{class in datasets.SyntheticData}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data:datasets.SyntheticData.SyntheticDataCreator}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{SyntheticDataCreator}}}{\emph{\DUrole{n}{num\_tasks}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{cellsPerTask}\DUrole{o}{=}\DUrole{default_value}{300}}, \emph{\DUrole{n}{drugsPerTask}\DUrole{o}{=}\DUrole{default_value}{10}}, \emph{\DUrole{n}{function}\DUrole{o}{=}\DUrole{default_value}{'gaussian'}}, \emph{\DUrole{n}{normalize}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{noise}\DUrole{o}{=}\DUrole{default_value}{0.1}}, \emph{\DUrole{n}{graph}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{test\_split}\DUrole{o}{=}\DUrole{default_value}{0.3}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
create synthetic data
\begin{description}
\item[{Args:}] \leavevmode\begin{description}
\item[{\sphinxcode{\sphinxupquote{num\_tasks}} (int): }] \leavevmode
number of tasks to create

\item[{\sphinxcode{\sphinxupquote{cellsPerTask}} (int): }] \leavevmode
number of cells to make for each task, this is dimension n of our ratings matrix

\item[{\sphinxcode{\sphinxupquote{drugsPerTask}} (int): }] \leavevmode
number of drugs to make for each task, this is dimension m of our ratings matrix

\item[{\sphinxcode{\sphinxupquote{sparsityPct}} (int (0,100)): }] \leavevmode
this is the amount of sparsity to put on the ratings matrix, the a higher percentage corresponds to a more sparse prediction matrix. Must be between 0 and 100.

\item[{\sphinxcode{\sphinxupquote{function}} (string): }] \leavevmode
gaussian or cosine indicating recipe for synthetic data

\item[{\sphinxcode{\sphinxupquote{normalize}} (boolean):}] \leavevmode
Boolean, whether data should be normalized

\item[{\sphinxcode{\sphinxupquote{test\_split}} (float {[}0,1{]}): }] \leavevmode
determines size of training and testing data

\item[{\sphinxcode{\sphinxupquote{noise}} (boolean):}] \leavevmode
amount of noise to use in creating synthetic data, the higher this value is the less correlation between generated tasks

\end{description}

\item[{Returns object with:}] \leavevmode\begin{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{self.datasets}} (list): }] \leavevmode
list of strings indicating dataset names

\item[{\sphinxcode{\sphinxupquote{self.data}} (dict): }] \leavevmode
multilevel dictionary with keys for train/test, then keys for x,y, then finally keys for dataset name.
ie: self.data{[}‘train’{]}{[}‘x’{]}{[}name1{]} gives training data for task 1. The models are built correspondingly.

\item[{\sphinxcode{\sphinxupquote{self.trainRatings}} (np.array): }] \leavevmode
array of ratings with shape (n,m) where n is the number of training cells / task and m is the number training of drugs / task

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{self.testRating}} (np.array): }] \leavevmode
array of ratings with shape (n,m) where n is the number of test cells / task and m is the number training of drugs / task

\end{description}

\end{description}
\index{create\_x\_and\_y() (SyntheticDataCreator method)@\spxentry{create\_x\_and\_y()}\spxextra{SyntheticDataCreator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data:datasets.SyntheticData.SyntheticDataCreator.create_x_and_y}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_x\_and\_y}}}{}{}
Wrapper for prepare data

\end{fulllineitems}

\index{generateCosSynthData() (SyntheticDataCreator method)@\spxentry{generateCosSynthData()}\spxextra{SyntheticDataCreator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data:datasets.SyntheticData.SyntheticDataCreator.generateCosSynthData}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generateCosSynthData}}}{\emph{\DUrole{n}{num\_tasks}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{ptsPerTask}\DUrole{o}{=}\DUrole{default_value}{1000}}, \emph{\DUrole{n}{noise}\DUrole{o}{=}\DUrole{default_value}{0.1}}, \emph{\DUrole{n}{graph}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Method used to generate synthetic data with the cosine function. 
This function selects set of uniform points on interval 0 to 1 and scales them each on intervals of 2c*pi
Where c is in range {[}1,nfeatures{]} user can set nfeatures in optional\_params.txt. Each feature maps 
to the same y value because they are shfited by one period. Then finally, some noise is added to each y,
in order to control the correlation between tasks. The more noise –> the less correlation.

\end{fulllineitems}

\index{generateSynthData() (SyntheticDataCreator method)@\spxentry{generateSynthData()}\spxextra{SyntheticDataCreator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data:datasets.SyntheticData.SyntheticDataCreator.generateSynthData}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generateSynthData}}}{\emph{\DUrole{n}{num\_tasks}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{ptsPerTask}\DUrole{o}{=}\DUrole{default_value}{1000}}, \emph{\DUrole{n}{noise}\DUrole{o}{=}\DUrole{default_value}{0.1}}, \emph{\DUrole{n}{graph}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Generates gaussian synthetic data. Coefficients apply common linear transformation to 
multivariate gaussian vectors. Number of gaussian vectors = nfeatures and cna be changed in optional\_params.txt
to add/remove features. Some noise added to coefficients to control correlation, similar to cosine function
more noise –> less correlation.

\end{fulllineitems}

\index{prepare\_data() (SyntheticDataCreator method)@\spxentry{prepare\_data()}\spxextra{SyntheticDataCreator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data:datasets.SyntheticData.SyntheticDataCreator.prepare_data}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{prepare\_data}}}{}{}
Run this public method to prepare data

\end{fulllineitems}

\index{set\_test\_split() (SyntheticDataCreator method)@\spxentry{set\_test\_split()}\spxextra{SyntheticDataCreator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data:datasets.SyntheticData.SyntheticDataCreator.set_test_split}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_test\_split}}}{\emph{\DUrole{n}{new\_test\_split}}}{}
Update the dict of test split.

\end{fulllineitems}

\index{shuffle\_and\_split() (SyntheticDataCreator method)@\spxentry{shuffle\_and\_split()}\spxextra{SyntheticDataCreator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data:datasets.SyntheticData.SyntheticDataCreator.shuffle_and_split}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{shuffle\_and\_split}}}{}{}
As stated in the name, shuffles and splits data again. Will fail if data has not been initialized

\end{fulllineitems}


\end{fulllineitems}



\section{Example}
\label{\detokenize{data:example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{SyntheticData} \PYG{k}{as} \PYG{n}{SD}

\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{SD}\PYG{o}{.}\PYG{n}{SyntheticDataCreator}\PYG{p}{(}\PYG{n}{num\PYGZus{}tasks}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{n}{cellsPerTask}\PYG{o}{=}\PYG{l+m+mi}{400}\PYG{p}{,} \PYG{n}{drugsPerTask}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{function}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cosine}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
             \PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{noise}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{graph}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{test\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{)}

\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{prepare\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

Now that we have instantiated dataset object, we can use dataset.data as a dictionary to access all the data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{task0\PYGZus{}train\PYGZus{}x} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{task0\PYGZus{}train\PYGZus{}y} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{task0\PYGZus{}test\PYGZus{}x} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{task0\PYGZus{}test\PYGZus{}y} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}


\chapter{Base Classes}
\label{\detokenize{base:base-classes}}\label{\detokenize{base::doc}}\index{BaseEstimator (class in methods.base)@\spxentry{BaseEstimator}\spxextra{class in methods.base}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base:methods.base.BaseEstimator}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{BaseEstimator}}}{\emph{\DUrole{n}{name}}, \emph{\DUrole{n}{type\_met}}, \emph{\DUrole{n}{paradigm}}, \emph{\DUrole{n}{output\_shape}}}{}
Abstract class representing a generic STL Method.
\index{evaluate() (BaseEstimator method)@\spxentry{evaluate()}\spxextra{BaseEstimator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base:methods.base.BaseEstimator.evaluate}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{abstract }}\sphinxbfcode{\sphinxupquote{evaluate}}}{}{}
Perform prediction.
\begin{description}
\item[{Args}] \leavevmode
\sphinxcode{\sphinxupquote{x}} (np.array):
np.array w/shape (nsamples,nfeatures)
\sphinxcode{\sphinxupquote{y}} (np.array):
np.array w/shape (nsamples,1)

\item[{Return}] \leavevmode
\sphinxcode{\sphinxupquote{results}} (np.array):
np.array of errors

\end{description}

\end{fulllineitems}

\index{fit() (BaseEstimator method)@\spxentry{fit()}\spxextra{BaseEstimator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base:methods.base.BaseEstimator.fit}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{abstract }}\sphinxbfcode{\sphinxupquote{fit}}}{}{}
fit model parameters
\begin{description}
\item[{Args}] \leavevmode
\sphinxcode{\sphinxupquote{x}} (np.array):
np.array w/shape (nsamples,nfeatures)
\sphinxcode{\sphinxupquote{y}} (np.array):
np.array w/shape (nsamples,1)

\end{description}

\end{fulllineitems}

\index{predict() (BaseEstimator method)@\spxentry{predict()}\spxextra{BaseEstimator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base:methods.base.BaseEstimator.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{abstract }}\sphinxbfcode{\sphinxupquote{predict}}}{}{}
Perform prediction.
\begin{description}
\item[{Args}] \leavevmode
\sphinxcode{\sphinxupquote{x}} (np.array):
np.array w/shape (nsamples,nfeatures)

\end{description}

\end{fulllineitems}

\index{set\_output\_directory() (BaseEstimator method)@\spxentry{set\_output\_directory()}\spxextra{BaseEstimator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base:methods.base.BaseEstimator.set_output_directory}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_output\_directory}}}{\emph{\DUrole{n}{output\_dir}}}{}
Set output folder path.
\begin{description}
\item[{Args:}] \leavevmode
\sphinxcode{\sphinxupquote{output\_dir}} (str): 
path to output directory.

\end{description}

\end{fulllineitems}

\index{set\_params() (BaseEstimator method)@\spxentry{set\_params()}\spxextra{BaseEstimator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base:methods.base.BaseEstimator.set_params}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{abstract }}\sphinxbfcode{\sphinxupquote{set\_params}}}{}{}
Set method’s parameters for optuna

\end{fulllineitems}


\end{fulllineitems}

\index{BaseMTLEstimator (class in methods.base)@\spxentry{BaseMTLEstimator}\spxextra{class in methods.base}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base:methods.base.BaseMTLEstimator}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{BaseMTLEstimator}}}{\emph{\DUrole{n}{name}}, \emph{\DUrole{n}{type\_met}}}{}
Base class for multitask learning estimators
\index{fit() (BaseMTLEstimator method)@\spxentry{fit()}\spxextra{BaseMTLEstimator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base:methods.base.BaseMTLEstimator.fit}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
fit model parameters
\begin{description}
\item[{Args}] \leavevmode
\sphinxcode{\sphinxupquote{x}} (dict):
dictionary with keys corresponding to feature vectors for each task eg: \{“CCLE”: np.array w/shape (nsamples,nfeatures)\}
\sphinxcode{\sphinxupquote{y}} (dict):
dictionary with keys corresponding to output vectors for each task eg: \{“CCLE”: np.array w/shape (nsamples,1)\}

\end{description}

\end{fulllineitems}

\index{predict() (BaseMTLEstimator method)@\spxentry{predict()}\spxextra{BaseMTLEstimator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{base:methods.base.BaseMTLEstimator.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
predict model parameters
\begin{description}
\item[{Args}] \leavevmode
\sphinxcode{\sphinxupquote{x}} (dict):
dictionary with keys corresponding to feature vectors for each task eg: \{“CCLE”: np.array w/shape (nsamples,nfeatures)\}

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Single Task Learning}
\label{\detokenize{stl:single-task-learning}}\label{\detokenize{stl::doc}}
These methods, mainly from Surprise (citation), offer clear recommender system benchmarks.
\index{SVD\_MF (class in methods.matrix\_factorization.MF)@\spxentry{SVD\_MF}\spxextra{class in methods.matrix\_factorization.MF}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{stl:methods.matrix_factorization.MF.SVD_MF}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{SVD\_MF}}}{\emph{\DUrole{n}{n\_factors}}, \emph{\DUrole{n}{n\_epochs}\DUrole{o}{=}\DUrole{default_value}{50}}, \emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'SVD\_MF'}}}{}
Bases: \sphinxcode{\sphinxupquote{methods.base.BaseSurpriseSTLEstimator}}

Matrix Factorization
\begin{description}
\item[{Args:}] \leavevmode\begin{description}
\item[{\sphinxcode{\sphinxupquote{n\_factors}} (int): }] \leavevmode
number of latent vectors/factors for matrix factorization

\item[{\sphinxcode{\sphinxupquote{n\_epochs}} (int): }] \leavevmode
Integer, The number of iteration of the SGD procedure. Default is 20

\end{description}

\end{description}

see \sphinxurl{https://surprise.readthedocs.io/en/stable/matrix\_factorization.html} for more info

\end{fulllineitems}

\index{NonNegative\_MF (class in methods.matrix\_factorization.MF)@\spxentry{NonNegative\_MF}\spxextra{class in methods.matrix\_factorization.MF}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{stl:methods.matrix_factorization.MF.NonNegative_MF}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{NonNegative\_MF}}}{\emph{\DUrole{n}{n\_factors}}, \emph{\DUrole{n}{n\_epochs}\DUrole{o}{=}\DUrole{default_value}{50}}, \emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'NonNegative\_MF'}}}{}
Bases: \sphinxcode{\sphinxupquote{methods.base.BaseSurpriseSTLEstimator}}

Nonnegative Matrix Factorization
\begin{description}
\item[{Args:}] \leavevmode\begin{description}
\item[{\sphinxcode{\sphinxupquote{n\_factors}} (int): }] \leavevmode
number of latent vectors/factors for matrix factorization

\item[{\sphinxcode{\sphinxupquote{n\_epochs}} (int): }] \leavevmode
Integer, The number of iteration of the SGD procedure. Default is 20

\end{description}

\end{description}

see \sphinxurl{https://surprise.readthedocs.io/en/stable/matrix\_factorization.html} for more info

\end{fulllineitems}

\index{KNN\_Basic (class in methods.knn.KNN)@\spxentry{KNN\_Basic}\spxextra{class in methods.knn.KNN}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{stl:methods.knn.KNN.KNN_Basic}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{KNN\_Basic}}}{\emph{\DUrole{n}{k}}, \emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'KNN\_Basic'}}, \emph{\DUrole{n}{sim\_options}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Bases: \sphinxcode{\sphinxupquote{methods.base.BaseSurpriseSTLEstimator}}
\begin{description}
\item[{Args:}] \leavevmode\begin{description}
\item[{\sphinxcode{\sphinxupquote{k}} (int):}] \leavevmode
number of neighbors

\item[{\sphinxcode{\sphinxupquote{sim\_options}} (optional):}] \leavevmode
option from surprise for a similarity metric

\end{description}

\end{description}

\end{fulllineitems}

\index{NN (class in methods.regressor.FFNN)@\spxentry{NN}\spxextra{class in methods.regressor.FFNN}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{stl:methods.regressor.FFNN.NN}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{NN}}}{\emph{\DUrole{n}{input\_dim}}, \emph{\DUrole{n}{arch}}, \emph{\DUrole{n}{activation}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

Vanilla Neural Network implementation
\begin{description}
\item[{Args:}] \leavevmode\begin{description}
\item[{\sphinxcode{\sphinxupquote{input}} (int): }] \leavevmode
dimension of input data

\item[{\sphinxcode{\sphinxupquote{arch}} (list):}] \leavevmode
list specifying architecture for each layer

\item[{\sphinxcode{\sphinxupquote{activation}} (string):}] \leavevmode
string specifying what activation to use ie: “ReLU” or “Sigmoid” or “TanH”

\end{description}

\end{description}
\index{forward() (NN method)@\spxentry{forward()}\spxextra{NN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{stl:methods.regressor.FFNN.NN.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Neural Collaborative Filtering}
\label{\detokenize{ncf:neural-collaborative-filtering}}\label{\detokenize{ncf::doc}}\index{Neural\_Collaborative\_Filtering\_Features (class in methods.matrix\_factorization.FeaturizedNCF)@\spxentry{Neural\_Collaborative\_Filtering\_Features}\spxextra{class in methods.matrix\_factorization.FeaturizedNCF}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{ncf:methods.matrix_factorization.FeaturizedNCF.Neural_Collaborative_Filtering_Features}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Neural\_Collaborative\_Filtering\_Features}}}{\emph{\DUrole{n}{hyperparams}}, \emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'Neural\_Collaborative\_Filtering\_Features'}}, \emph{\DUrole{n}{type\_met}\DUrole{o}{=}\DUrole{default_value}{'feature\_based'}}, \emph{\DUrole{n}{paradigm}\DUrole{o}{=}\DUrole{default_value}{'stl'}}, \emph{\DUrole{n}{output\_shape}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{warm\_start}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{learner}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{reg\_mf}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{num\_factors}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Bases: {\hyperref[\detokenize{base:methods.base.BaseEstimator}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{methods.base.BaseEstimator}}}}}

Neural Collaborative Filtering adapted from \sphinxurl{https://github.com/hexiangnan/neural\_collaborative\_filtering}

Combines matrix factorization and Multilayer Perceptron. \sphinxstylestrong{Uses cell and drug features}
\begin{description}
\item[{Args:}] \leavevmode\begin{description}
\item[{\sphinxcode{\sphinxupquote{hyperparams}} (dict):}] \leavevmode\begin{quote}

dictionary containing keys for each hyperparameter.
\begin{description}
\item[{\sphinxcode{\sphinxupquote{num\_epochs}} (int):}] \leavevmode
number of epochs to train for

\item[{\sphinxcode{\sphinxupquote{batch\_size}} (int):}] \leavevmode
size of each batch in training epochs

\item[{\sphinxcode{\sphinxupquote{mf\_dim}} (int):}] \leavevmode
number of factors to be used by matrix factorization

\item[{\sphinxcode{\sphinxupquote{layers}} (list):}] \leavevmode
list describing architecure for multilayer perceptron. ie: {[}32,16,8{]}

\item[{\sphinxcode{\sphinxupquote{reg\_mf}} (float):}] \leavevmode
regularization penalty for matrix factorization

\item[{\sphinxcode{\sphinxupquote{reg\_layer}} (list):}] \leavevmode
list describing architecure for regularizing multilayer perceptron. ie: {[}32,16,8{]}. Must match length of layers

\item[{\sphinxcode{\sphinxupquote{num\_negatives}} :}] \leavevmode
ignore this, deprecated

\item[{\sphinxcode{\sphinxupquote{learning\_rate}} (int):}] \leavevmode
learning rate for gradient descent weight optimization

\item[{\sphinxcode{\sphinxupquote{learner}} (string):}] \leavevmode
name of learner to use. Options are sgd, adam, rmsprop, decayed sgd, scheduled sgd, adagrad

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{warmstart}} (boolean):}] \leavevmode
whether to instantiate a model for each task or keep training the same one.

\end{description}

\end{description}

Ignore the other arguments, they are there to pass in hyperparameters with optuna.

\end{description}

\end{fulllineitems}

\index{Neural\_Collaborative\_Filtering (class in methods.matrix\_factorization.CustomInputNCF)@\spxentry{Neural\_Collaborative\_Filtering}\spxextra{class in methods.matrix\_factorization.CustomInputNCF}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{ncf:methods.matrix_factorization.CustomInputNCF.Neural_Collaborative_Filtering}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Neural\_Collaborative\_Filtering}}}{\emph{\DUrole{n}{hyperparams}}, \emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'Neural\_Collaborative\_Filtering'}}, \emph{\DUrole{n}{type\_met}\DUrole{o}{=}\DUrole{default_value}{'non\_feature\_based'}}, \emph{\DUrole{n}{paradigm}\DUrole{o}{=}\DUrole{default_value}{'stl'}}, \emph{\DUrole{n}{output\_shape}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{warm\_start}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{learner}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{reg\_mf}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{num\_factors}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Bases: {\hyperref[\detokenize{base:methods.base.BaseEstimator}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{methods.base.BaseEstimator}}}}}

Neural Collaborative Filtering adapted for regression from \sphinxurl{https://github.com/hexiangnan/neural\_collaborative\_filtering}

Combines matrix factorization and Multilayer Perceptron. \sphinxstylestrong{Does not use cell and drug features}
\begin{description}
\item[{Args:}] \leavevmode\begin{description}
\item[{\sphinxcode{\sphinxupquote{hyperparams}} (dict):}] \leavevmode\begin{quote}

dictionary containing keys for each hyperparameter.
\begin{description}
\item[{\sphinxcode{\sphinxupquote{num\_epochs}} (int):}] \leavevmode
number of epochs to train for

\item[{\sphinxcode{\sphinxupquote{batch\_size}} (int):}] \leavevmode
size of each batch in training epochs

\item[{\sphinxcode{\sphinxupquote{mf\_dim}} (int):}] \leavevmode
number of factors to be used by matrix factorization

\item[{\sphinxcode{\sphinxupquote{layers}} (list):}] \leavevmode
list describing architecure for multilayer perceptron. ie: {[}32,16,8{]}

\item[{\sphinxcode{\sphinxupquote{reg\_mf}} (float):}] \leavevmode
regularization penalty for matrix factorization

\item[{\sphinxcode{\sphinxupquote{reg\_layer}} (list):}] \leavevmode
list describing architecure for regularizing multilayer perceptron. ie: {[}32,16,8{]}. Must match length of layers

\item[{\sphinxcode{\sphinxupquote{num\_negatives}} :}] \leavevmode
ignore this, deprecated

\item[{\sphinxcode{\sphinxupquote{learning\_rate}} (int):}] \leavevmode
learning rate for gradient descent weight optimization

\item[{\sphinxcode{\sphinxupquote{learner}} (string):}] \leavevmode
name of learner to use. Options are sgd, adam, rmsprop, decayed sgd, scheduled sgd, adagrad

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{warmstart}} (boolean):}] \leavevmode
whether to instantiate a model for each task or keep training the same one.

\end{description}

\end{description}

Ignore the other arguments, they are there to pass in hyperparameters with optuna.

\end{description}

\end{fulllineitems}

\index{Neural\_Collaborative\_Filtering\_FeaturesMTLMLP (class in methods.mtl.NCF\_MTL)@\spxentry{Neural\_Collaborative\_Filtering\_FeaturesMTLMLP}\spxextra{class in methods.mtl.NCF\_MTL}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{ncf:methods.mtl.NCF_MTL.Neural_Collaborative_Filtering_FeaturesMTLMLP}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Neural\_Collaborative\_Filtering\_FeaturesMTLMLP}}}{\emph{\DUrole{n}{hyperparams}}, \emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'Neural\_Collaborative\_Filtering\_Features'}}, \emph{\DUrole{n}{type\_met}\DUrole{o}{=}\DUrole{default_value}{'feature\_based'}}, \emph{\DUrole{n}{paradigm}\DUrole{o}{=}\DUrole{default_value}{'mtl'}}, \emph{\DUrole{n}{output\_shape}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{warm\_start}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{learner}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{reg\_mf}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{num\_factors}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Bases: {\hyperref[\detokenize{base:methods.base.BaseMTLEstimator}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{methods.base.BaseMTLEstimator}}}}}

NCF adapted for multitask model that first trains shared MLP on pooled data, then trains a seperate GMF model for each task.

Combines matrix factorization and Multilayer Perceptron. \sphinxstylestrong{Uses cell and drug features}
\begin{description}
\item[{Args:}] \leavevmode\begin{description}
\item[{\sphinxcode{\sphinxupquote{hyperparams}} (dict):}] \leavevmode\begin{quote}

dictionary containing keys for each hyperparameter.
\sphinxcode{\sphinxupquote{num\_epochs}} (int):
\begin{quote}

number of epochs to train for
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{batch\_size}} (int):}] \leavevmode
size of each batch in training epochs

\item[{\sphinxcode{\sphinxupquote{mf\_dim}} (int):}] \leavevmode
number of factors to be used by matrix factorization

\item[{\sphinxcode{\sphinxupquote{layers}} (list):}] \leavevmode
list describing architecure for multilayer perceptron. ie: {[}32,16,8{]}

\item[{\sphinxcode{\sphinxupquote{reg\_mf}} (float):}] \leavevmode
regularization penalty for matrix factorization

\item[{\sphinxcode{\sphinxupquote{reg\_layer}} (list):}] \leavevmode
list describing architecure for regularizing multilayer perceptron. ie: {[}32,16,8{]}. Must match length of layers

\item[{\sphinxcode{\sphinxupquote{learning\_rate}} (int):}] \leavevmode
learning rate for gradient descent weight optimization

\item[{\sphinxcode{\sphinxupquote{learner}} (string):}] \leavevmode
name of learner to use. Options are sgd, adam, rmsprop, decayed sgd, scheduled sgd, adagrad

\item[{\sphinxcode{\sphinxupquote{mlp\_lr}} (float):}] \leavevmode
(0,1) float for learning rate for pooled MLP model.

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{warmstart}} (boolean):}] \leavevmode
whether to instantiate a model for each task or keep training the same one.

\end{description}

\end{description}

Ignore the other arguments, they are there to pass in hyperparameters with optuna.

\end{description}

\end{fulllineitems}

\index{Neural\_Collaborative\_Filtering\_FeaturesMTLMF (class in methods.mtl.NCF\_MTL)@\spxentry{Neural\_Collaborative\_Filtering\_FeaturesMTLMF}\spxextra{class in methods.mtl.NCF\_MTL}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{ncf:methods.mtl.NCF_MTL.Neural_Collaborative_Filtering_FeaturesMTLMF}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Neural\_Collaborative\_Filtering\_FeaturesMTLMF}}}{\emph{\DUrole{n}{hyperparams}}, \emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'Neural\_Collaborative\_Filtering\_Features'}}, \emph{\DUrole{n}{type\_met}\DUrole{o}{=}\DUrole{default_value}{'feature\_based'}}, \emph{\DUrole{n}{paradigm}\DUrole{o}{=}\DUrole{default_value}{'mtl'}}, \emph{\DUrole{n}{output\_shape}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{warm\_start}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{learner}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{reg\_mf}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{num\_factors}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Bases: {\hyperref[\detokenize{base:methods.base.BaseMTLEstimator}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{methods.base.BaseMTLEstimator}}}}}

NCF adapted for multitask model that first trains shared MF on pooled data, then trains a seperate MLP model for each task.

Combines matrix factorization and Multilayer Perceptron. \sphinxstylestrong{Uses cell and drug features}
\begin{description}
\item[{Args:}] \leavevmode\begin{description}
\item[{\sphinxcode{\sphinxupquote{hyperparams}} (dict):}] \leavevmode\begin{quote}

dictionary containing keys for each hyperparameter.
\begin{description}
\item[{\sphinxcode{\sphinxupquote{num\_epochs}} (int):}] \leavevmode
number of epochs to train for

\item[{\sphinxcode{\sphinxupquote{batch\_size}} (int):}] \leavevmode
size of each batch in training epochs

\item[{\sphinxcode{\sphinxupquote{mf\_dim}} (int):}] \leavevmode
number of factors to be used by matrix factorization

\item[{\sphinxcode{\sphinxupquote{layers}} (list):}] \leavevmode
list describing architecure for multilayer perceptron. ie: {[}32,16,8{]}

\item[{\sphinxcode{\sphinxupquote{reg\_mf}} (float):}] \leavevmode
regularization penalty for matrix factorization

\item[{\sphinxcode{\sphinxupquote{reg\_layer}} (list):}] \leavevmode
list describing architecure for regularizing multilayer perceptron. ie: {[}32,16,8{]}. Must match length of layers

\item[{\sphinxcode{\sphinxupquote{learning\_rate}} (int):}] \leavevmode
learning rate for gradient descent weight optimization

\item[{\sphinxcode{\sphinxupquote{learner}} (string):}] \leavevmode
name of learner to use. Options are sgd, adam, rmsprop, decayed sgd, scheduled sgd, adagrad

\item[{\sphinxcode{\sphinxupquote{mf\_lr}} (float):}] \leavevmode
(0,1) float for learning rate for pooled MF model.

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{warmstart}} (boolean):}] \leavevmode
whether to instantiate a model for each task or keep training the same one.

\end{description}

\end{description}

Ignore the other arguments, they are there to pass in hyperparameters with optuna.

\end{description}

\end{fulllineitems}



\chapter{Gaussian Processes}
\label{\detokenize{gp:gaussian-processes}}\label{\detokenize{gp::doc}}

\section{Single Task GPs}
\label{\detokenize{gp:single-task-gps}}\index{ExactGPRegression (class in methods.regressor.ExactGP)@\spxentry{ExactGPRegression}\spxextra{class in methods.regressor.ExactGP}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{gp:methods.regressor.ExactGP.ExactGPRegression}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ExactGPRegression}}}{\emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'ExactGP'}}, \emph{\DUrole{n}{num\_iters}\DUrole{o}{=}\DUrole{default_value}{50}}, \emph{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{0.1}}, \emph{\DUrole{n}{noise\_covar}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{length\_scale}\DUrole{o}{=}\DUrole{default_value}{100.0}}, \emph{\DUrole{n}{output\_scale}\DUrole{o}{=}\DUrole{default_value}{1.0}}}{}
Bases: \sphinxcode{\sphinxupquote{methods.base.BaseOwnSTLEstimator}}

Exact GP, Gaussian Process evaluated at all training points
\begin{description}
\item[{Args:}] \leavevmode\begin{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{name}} (optional, string):}] \leavevmode
model name

\item[{\sphinxcode{\sphinxupquote{num\_iters}} (int):}] \leavevmode
number of iterations for Gaussian Process

\item[{\sphinxcode{\sphinxupquote{learning\_rate}} (int):}] \leavevmode
learning rate for conjugate gradient. recommended around .1 or .01

\item[{\sphinxcode{\sphinxupquote{noise\_covar}} (float):}] \leavevmode
hyperparamter, noise assumed in the data

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{lengthscale}} (float):}] \leavevmode
hyperparameter, magnitude relative to assumed correlation in data

\item[{\sphinxcode{\sphinxupquote{output\_scale}} (optional, float):}] \leavevmode
scaling parameter

\end{description}

\end{description}

\end{fulllineitems}

\index{ExactGPCompositeKernelRegression (class in methods.regressor.ExactGPCompositeKernel)@\spxentry{ExactGPCompositeKernelRegression}\spxextra{class in methods.regressor.ExactGPCompositeKernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{gp:methods.regressor.ExactGPCompositeKernel.ExactGPCompositeKernelRegression}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ExactGPCompositeKernelRegression}}}{\emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'ExactGPCompositeKernel'}}, \emph{\DUrole{n}{num\_iters}\DUrole{o}{=}\DUrole{default_value}{50}}, \emph{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{0.1}}, \emph{\DUrole{n}{noise\_covar}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{length\_scale\_cell}\DUrole{o}{=}\DUrole{default_value}{100.0}}, \emph{\DUrole{n}{output\_scale\_cell}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{length\_scale\_drug}\DUrole{o}{=}\DUrole{default_value}{100.0}}, \emph{\DUrole{n}{output\_scale\_drug}\DUrole{o}{=}\DUrole{default_value}{1.0}}}{}
Bases: \sphinxcode{\sphinxupquote{methods.base.BaseOwnSTLEstimator}}

Exact GP where seperate Kernels are evaluated for drugs and cells and then muliplied or added to make a shared kernel, Gaussian Process evaluated at all training points
\begin{description}
\item[{Args:}] \leavevmode\begin{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{name}} (optional, string):}] \leavevmode
model name

\item[{\sphinxcode{\sphinxupquote{num\_iters}} (int):}] \leavevmode
number of iterations for Gaussian Process

\item[{\sphinxcode{\sphinxupquote{learning\_rate}} (int):}] \leavevmode
learning rate for conjugate gradient. recommended around .1 or .01

\item[{\sphinxcode{\sphinxupquote{noise\_covar}} (float):}] \leavevmode
hyperparamter, noise assumed in the data

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{length\_scale\_cell}} (float):}] \leavevmode
hyperparameter, magnitude relative to assumed correlation in \sphinxstylestrong{cell} data

\item[{\sphinxcode{\sphinxupquote{length\_scale\_drug}} (float):}] \leavevmode
hyperparameter, magnitude relative to assumed correlation in \sphinxstylestrong{drug} data

\item[{\sphinxcode{\sphinxupquote{output\_scale\_drug}} (optional, float):}] \leavevmode
scaling parameter for \sphinxstylestrong{drug} data

\item[{\sphinxcode{\sphinxupquote{output\_scale\_cell}} (optional, float):}] \leavevmode
scaling parameter for \sphinxstylestrong{cell} data

\end{description}

\end{description}

\end{fulllineitems}

\index{SparseGPRegression (class in methods.regressor.SparseGP)@\spxentry{SparseGPRegression}\spxextra{class in methods.regressor.SparseGP}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{gp:methods.regressor.SparseGP.SparseGPRegression}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{SparseGPRegression}}}{\emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'SparseGP'}}, \emph{\DUrole{n}{num\_iters}\DUrole{o}{=}\DUrole{default_value}{50}}, \emph{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{0.1}}, \emph{\DUrole{n}{noise\_covar}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{length\_scale}\DUrole{o}{=}\DUrole{default_value}{100.0}}, \emph{\DUrole{n}{output\_scale}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{n\_inducing\_points}\DUrole{o}{=}\DUrole{default_value}{500}}, \emph{\DUrole{n}{use\_initial}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Bases: \sphinxcode{\sphinxupquote{methods.base.BaseOwnSTLEstimator}}

Sparse GP, Gaussian Process evaluated only at N inducing points sampled from training points
\begin{description}
\item[{Args:}] \leavevmode\begin{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{name}} (optional, string):}] \leavevmode
model name

\item[{\sphinxcode{\sphinxupquote{num\_iters}} (int):}] \leavevmode
number of iterations for Gaussian Process

\item[{\sphinxcode{\sphinxupquote{learning\_rate}} (int):}] \leavevmode
learning rate for conjugate gradient. recommended around .1 or .01

\item[{\sphinxcode{\sphinxupquote{noise\_covar}} (float):}] \leavevmode
hyperparamter, noise assumed in the data

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{lengthscale}} (float):}] \leavevmode
hyperparameter, magnitude relative to assumed correlation in data

\item[{\sphinxcode{\sphinxupquote{output\_scale}} (optional, float):}] \leavevmode
scaling parameter

\item[{\sphinxcode{\sphinxupquote{n\_inducing\_points}} (optional, int):}] \leavevmode
number of training points to sample from for Gaussian Process

\end{description}

\end{description}

\end{fulllineitems}

\index{SparseGPCompositeKernelRegression (class in methods.regressor.SparseGPCompositeKernel)@\spxentry{SparseGPCompositeKernelRegression}\spxextra{class in methods.regressor.SparseGPCompositeKernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{gp:methods.regressor.SparseGPCompositeKernel.SparseGPCompositeKernelRegression}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{SparseGPCompositeKernelRegression}}}{\emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'SparseGPCompositeKernel'}}, \emph{\DUrole{n}{num\_iters}\DUrole{o}{=}\DUrole{default_value}{50}}, \emph{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{0.1}}, \emph{\DUrole{n}{noise\_covar}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{length\_scale\_cell}\DUrole{o}{=}\DUrole{default_value}{100.0}}, \emph{\DUrole{n}{output\_scale\_cell}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{length\_scale\_drug}\DUrole{o}{=}\DUrole{default_value}{100.0}}, \emph{\DUrole{n}{output\_scale\_drug}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{n\_inducing\_points}\DUrole{o}{=}\DUrole{default_value}{500}}}{}
Bases: \sphinxcode{\sphinxupquote{methods.base.BaseOwnSTLEstimator}}

Sparse GP where seperate Kernels are evaluated for drugs and cells and then muliplied or added to make a shared kernel, Gaussian Process evaluated at only n\_inducing training points
\begin{description}
\item[{Args:}] \leavevmode\begin{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{name}} (optional, string):}] \leavevmode
model name

\item[{\sphinxcode{\sphinxupquote{num\_iters}} (int):}] \leavevmode
number of iterations for Gaussian Process

\item[{\sphinxcode{\sphinxupquote{learning\_rate}} (int):}] \leavevmode
learning rate for conjugate gradient. recommended around .1 or .01

\item[{\sphinxcode{\sphinxupquote{noise\_covar}} (float):}] \leavevmode
hyperparamter, noise assumed in the data

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{length\_scale\_cell}} (float):}] \leavevmode
hyperparameter, magnitude relative to assumed correlation in \sphinxstylestrong{cell} data

\item[{\sphinxcode{\sphinxupquote{length\_scale\_drug}} (float):}] \leavevmode
hyperparameter, magnitude relative to assumed correlation in \sphinxstylestrong{drug} data

\item[{\sphinxcode{\sphinxupquote{output\_scale\_drug}} (optional, float):}] \leavevmode
scaling parameter for \sphinxstylestrong{drug} data

\item[{\sphinxcode{\sphinxupquote{output\_scale\_cell}} (optional, float):}] \leavevmode
scaling parameter for \sphinxstylestrong{cell} data

\item[{\sphinxcode{\sphinxupquote{n\_inducing\_points}} (optional, int):}] \leavevmode
number of training points to sample from for Gaussian Process

\end{description}

\end{description}

\end{fulllineitems}



\section{MultiTask GPs}
\label{\detokenize{gp:multitask-gps}}\index{HadamardMTL (class in methods.mtl.MTL\_GP)@\spxentry{HadamardMTL}\spxextra{class in methods.mtl.MTL\_GP}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{gp:methods.mtl.MTL_GP.HadamardMTL}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{HadamardMTL}}}{\emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'HadamardMTL'}}, \emph{\DUrole{n}{num\_iters}\DUrole{o}{=}\DUrole{default_value}{50}}, \emph{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{0.1}}, \emph{\DUrole{n}{noise\_covar}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{length\_scale}\DUrole{o}{=}\DUrole{default_value}{100.0}}, \emph{\DUrole{n}{output\_scale}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{n\_inducing\_points}\DUrole{o}{=}\DUrole{default_value}{500}}, \emph{\DUrole{n}{composite}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{validate}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{bias}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{stabilize}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{use\_initial}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Bases: {\hyperref[\detokenize{base:methods.base.BaseMTLEstimator}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{methods.base.BaseMTLEstimator}}}}}

pipeline suited implementation of
\sphinxurl{https://docs.gpytorch.ai/en/v1.1.1/examples/03\_Multitask\_Exact\_GPs/Hadamard\_Multitask\_GP\_Regression.html}
\begin{description}
\item[{Args:}] \leavevmode\begin{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{name}} (optional, string):}] \leavevmode
model name

\item[{\sphinxcode{\sphinxupquote{num\_iters}} (int):}] \leavevmode
number of iterations for Gaussian Process

\item[{\sphinxcode{\sphinxupquote{learning\_rate}} (int):}] \leavevmode
learning rate for conjugate gradient. recommended around .1 or .01

\item[{\sphinxcode{\sphinxupquote{noise\_covar}} (float):}] \leavevmode
hyperparamter, noise assumed in the data

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{lengthscale}} (float):}] \leavevmode
hyperparameter, magnitude relative to assumed correlation in data

\item[{\sphinxcode{\sphinxupquote{output\_scale}} (optional, float):}] \leavevmode
scaling parameter

\item[{\sphinxcode{\sphinxupquote{n\_inducing\_points}} (optional, int):}] \leavevmode
number of training points to sample from for Gaussian Process

\item[{\sphinxcode{\sphinxupquote{composite}} (bool):}] \leavevmode
whether to use composite kernel or not

\item[{\sphinxcode{\sphinxupquote{validate}} (bool):}] \leavevmode
whether to produce validation curve data as well during training

\item[{\sphinxcode{\sphinxupquote{bias}} (bool):}] \leavevmode
whether to add bias term for each dataset

\item[{\sphinxcode{\sphinxupquote{stabilize}} (bool):}] \leavevmode
whether to stabilize loss at the end

\item[{\sphinxcode{\sphinxupquote{use\_initial}} (bool):}] \leavevmode
whether to even use initial parameters

\end{description}

\end{description}

\end{fulllineitems}

\index{GPyFullMTL (class in methods.mtl.MTL\_GP)@\spxentry{GPyFullMTL}\spxextra{class in methods.mtl.MTL\_GP}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{gp:methods.mtl.MTL_GP.GPyFullMTL}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{GPyFullMTL}}}{\emph{\DUrole{n}{name}\DUrole{o}{=}\DUrole{default_value}{'fullGP'}}, \emph{\DUrole{n}{num\_iters}\DUrole{o}{=}\DUrole{default_value}{50}}, \emph{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{0.1}}, \emph{\DUrole{n}{noise\_covar}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{length\_scale}\DUrole{o}{=}\DUrole{default_value}{100.0}}, \emph{\DUrole{n}{output\_scale}\DUrole{o}{=}\DUrole{default_value}{1.0}}, \emph{\DUrole{n}{n\_inducing\_points}\DUrole{o}{=}\DUrole{default_value}{500}}, \emph{\DUrole{n}{use\_initial}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{num\_tasks}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{validate}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Bases: {\hyperref[\detokenize{base:methods.base.BaseMTLEstimator}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{methods.base.BaseMTLEstimator}}}}}

Adapted of Full MultiTask GP model from GpyTorch
\begin{description}
\item[{Args:}] \leavevmode\begin{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{name}} (optional, string):}] \leavevmode
model name

\item[{\sphinxcode{\sphinxupquote{num\_iters}} (int):}] \leavevmode
number of iterations for Gaussian Process

\item[{\sphinxcode{\sphinxupquote{learning\_rate}} (int):}] \leavevmode
learning rate for conjugate gradient. recommended around .1 or .01

\item[{\sphinxcode{\sphinxupquote{noise\_covar}} (float):}] \leavevmode
hyperparamter, noise assumed in the data

\end{description}
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{lengthscale}} (float):}] \leavevmode
hyperparameter, magnitude relative to assumed correlation in data

\item[{\sphinxcode{\sphinxupquote{output\_scale}} (optional, float):}] \leavevmode
scaling parameter

\item[{\sphinxcode{\sphinxupquote{n\_inducing\_points}} (optional, int):}] \leavevmode
number of training points to sample from for Gaussian Process

\item[{\sphinxcode{\sphinxupquote{bias\_only}} (bool):}] \leavevmode
deprecated. Do not use.

\item[{\sphinxcode{\sphinxupquote{num\_tasks}} (int):}] \leavevmode
number of tasks you are giving model, should be equal to number of datasets

\end{description}

\end{description}

\end{fulllineitems}



\chapter{Optuna Example Hyperparameter Optimization KNN, SVD, NNMF}
\label{\detokenize{Optuna_Walkthrough:Optuna-Example-Hyperparameter-Optimization-KNN,-SVD,-NNMF}}\label{\detokenize{Optuna_Walkthrough::doc}}
In order for the code below to work for a different method/model, the model should have member functions like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{get\PYGZus{}hyper\PYGZus{}params}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{hparams} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}factors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{integer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
               \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rho\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loguniform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
               \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rho\PYGZus{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loguniform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}
    \PYG{k}{return} \PYG{n}{hparams}

\PYG{k}{def} \PYG{n+nf}{set\PYGZus{}hyper\PYGZus{}params}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}factors} \PYG{o}{=} \PYG{n}{kwargs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}factors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{rho1} \PYG{o}{=} \PYG{n}{kwargs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rho\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{rho2} \PYG{o}{=} \PYG{n}{kwargs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rho\PYGZus{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[2]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{os}\PYG{o}{,} \PYG{n+nn}{sys}
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k+kn}{import} \PYG{n+nn}{hp\PYGZus{}optimization} \PYG{k}{as} \PYG{n+nn}{hopt}
\PYG{k+kn}{from} \PYG{n+nn}{optuna}\PYG{n+nn}{.}\PYG{n+nn}{visualization} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}optimization\PYGZus{}history}\PYG{p}{,} \PYGZbs{}
                                 \PYG{n}{plot\PYGZus{}intermediate\PYGZus{}values}\PYG{p}{,} \PYGZbs{}
                                 \PYG{n}{plot\PYGZus{}contour}

\PYG{k+kn}{from} \PYG{n+nn}{design} \PYG{k+kn}{import} \PYG{n}{ModelTraining}
\PYG{k+kn}{from} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{matrix\PYGZus{}factorization}\PYG{n+nn}{.}\PYG{n+nn}{MF\PYGZus{}STL} \PYG{k+kn}{import} \PYG{n}{MF\PYGZus{}STL}
\PYG{k+kn}{from} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{matrix\PYGZus{}factorization}\PYG{n+nn}{.}\PYG{n+nn}{MF} \PYG{k+kn}{import} \PYG{n}{SVD\PYGZus{}MF}\PYG{p}{,} \PYG{n}{NonNegative\PYGZus{}MF}
\PYG{k+kn}{from} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{knn}\PYG{n+nn}{.}\PYG{n+nn}{KNN} \PYG{k+kn}{import} \PYG{n}{KNN\PYGZus{}Normalized}
\PYG{k+kn}{from} \PYG{n+nn}{shutil} \PYG{k+kn}{import} \PYG{n}{copyfile}
\PYG{k+kn}{from} \PYG{n+nn}{UTILS}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{datasetParams2str}
\PYG{k+kn}{from} \PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{SyntheticData} \PYG{k}{as} \PYG{n}{SD}

\PYG{n}{outdir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../outputs/experiment\PYGZus{}004x}\PYG{l+s+s1}{\PYGZsq{}} \PYG{c+c1}{\PYGZsh{} make sure that it lines up with the experiment\PYGZsq{}s filename}
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{n}{outdir}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{makedirs}\PYG{p}{(}\PYG{n}{outdir}\PYG{p}{)}
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[3]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{SD}\PYG{o}{.}\PYG{n}{SyntheticDataCreator}\PYG{p}{(}\PYG{n}{num\PYGZus{}tasks}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{n}{cellsPerTask}\PYG{o}{=}\PYG{l+m+mi}{400}\PYG{p}{,} \PYG{n}{drugsPerTask}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{function}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cosine}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
             \PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{noise}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{graph}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{test\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{)}
\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{prepare\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{methods}  \PYG{o}{=} \PYG{p}{[}\PYG{n}{KNN\PYGZus{}Normalized}\PYG{p}{(}\PYG{n}{k}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{n}{SVD\PYGZus{}MF}\PYG{p}{(}\PYG{n}{n\PYGZus{}factors}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{,} \PYG{n}{NonNegative\PYGZus{}MF}\PYG{p}{(}\PYG{n}{n\PYGZus{}factors}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{k}{for} \PYG{n}{method} \PYG{o+ow}{in} \PYG{n}{methods}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{i} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{study} \PYG{o}{=} \PYG{n}{hopt}\PYG{o}{.}\PYG{n}{optimize\PYGZus{}hyper\PYGZus{}params}\PYG{p}{(}\PYG{n}{method}\PYG{p}{,} \PYG{n}{dataset}\PYG{p}{,}\PYG{n}{n\PYGZus{}trials}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
        \PYG{n}{i} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{study} \PYG{o}{=} \PYG{n}{hopt}\PYG{o}{.}\PYG{n}{optimize\PYGZus{}hyper\PYGZus{}params}\PYG{p}{(}\PYG{n}{method}\PYG{p}{,} \PYG{n}{dataset}\PYG{p}{,}\PYG{n}{n\PYGZus{}trials}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}
    \PYG{n}{plot\PYGZus{}optimization\PYGZus{}history}\PYG{p}{(}\PYG{n}{study}\PYG{p}{)}
    \PYG{n}{plot\PYGZus{}intermediate\PYGZus{}values}\PYG{p}{(}\PYG{n}{study}\PYG{p}{)}
    \PYG{n}{plot\PYGZus{}contour}\PYG{p}{(}\PYG{n}{study}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{best params for }\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{+} \PYG{n}{method}\PYG{o}{.}\PYG{n}{name} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{study}\PYG{o}{.}\PYG{n}{best\PYGZus{}params}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} copy the study, i.e. hyperparam trials}
    \PYG{n}{dataset\PYGZus{}str} \PYG{o}{=} \PYG{n}{datasetParams2str}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}dict\PYGZus{}\PYGZus{}}\PYG{p}{)}
    \PYG{n}{study\PYGZus{}name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZus{}}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{method}\PYG{o}{.}\PYG{n}{name}\PYG{p}{,}\PYG{n}{dataset\PYGZus{}str}\PYG{p}{)}
    \PYG{n}{storage}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hyperparam\PYGZus{}experiments/}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{.db}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{study\PYGZus{}name}\PYG{p}{)}
    \PYG{n}{copyfile}\PYG{p}{(}\PYG{n}{storage}\PYG{p}{,} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{outdir}\PYG{p}{,}\PYG{n}{study\PYGZus{}name} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.db}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}
\end{sphinxVerbatim}
}




\chapter{Visualizing High\sphinxhyphen{}dimensional Parameter Relationships}
\label{\detokenize{Optuna_VizWalkthrough:Visualizing-High-dimensional-Parameter-Relationships}}\label{\detokenize{Optuna_VizWalkthrough::doc}}
This notebook demonstrates various visualizations of studies in Optuna. The hyperparameters of a neural network trained to classify images are optimized and the resulting study is then visualized using these features.

\sphinxstylestrong{Note:} If a parameter contains missing values, a trial with missing values is not plotted.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{c+c1}{\PYGZsh{} If you run this notebook on Google Colaboratory, uncomment the below to install Optuna.}
\PYG{c+c1}{\PYGZsh{}! pip install \PYGZhy{}\PYGZhy{}quiet optuna}
\end{sphinxVerbatim}
}

\sphinxstylestrong{SOURCE:} \sphinxurl{https://github.com/optuna/optuna/blob/master/examples/visualization/plot\_study.ipynb}


\section{Preparing the Dataset}
\label{\detokenize{Optuna_VizWalkthrough:Preparing-the-Dataset}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{fetch\PYGZus{}openml}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}

\PYG{n}{mnist} \PYG{o}{=} \PYG{n}{fetch\PYGZus{}openml}\PYG{p}{(}\PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Fashion\PYGZhy{}MNIST}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{version}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{classes} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{set}\PYG{p}{(}\PYG{n}{mnist}\PYG{o}{.}\PYG{n}{target}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} For demonstrational purpose, only use a subset of the dataset.}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{4000}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{n}{n\PYGZus{}samples}\PYG{p}{]}
\PYG{n}{target} \PYG{o}{=} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{target}\PYG{p}{[}\PYG{p}{:}\PYG{n}{n\PYGZus{}samples}\PYG{p}{]}

\PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{x\PYGZus{}valid}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}valid} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{target}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Defining the Objective Function}
\label{\detokenize{Optuna_VizWalkthrough:Defining-the-Objective-Function}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neural\PYGZus{}network} \PYG{k+kn}{import} \PYG{n}{MLPClassifier}

\PYG{k}{def} \PYG{n+nf}{objective}\PYG{p}{(}\PYG{n}{trial}\PYG{p}{)}\PYG{p}{:}

    \PYG{n}{clf} \PYG{o}{=} \PYG{n}{MLPClassifier}\PYG{p}{(}
        \PYG{n}{hidden\PYGZus{}layer\PYGZus{}sizes}\PYG{o}{=}\PYG{n+nb}{tuple}\PYG{p}{(}\PYG{p}{[}\PYG{n}{trial}\PYG{o}{.}\PYG{n}{suggest\PYGZus{}int}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}units\PYGZus{}l}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{i}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{32}\PYG{p}{,} \PYG{l+m+mi}{64}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{learning\PYGZus{}rate\PYGZus{}init}\PYG{o}{=}\PYG{n}{trial}\PYG{o}{.}\PYG{n}{suggest\PYGZus{}float}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lr\PYGZus{}init}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{1e\PYGZhy{}5}\PYG{p}{,} \PYG{l+m+mf}{1e\PYGZhy{}1}\PYG{p}{,} \PYG{n}{log}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{,}
    \PYG{p}{)}

    \PYG{k}{for} \PYG{n}{step} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{clf}\PYG{o}{.}\PYG{n}{partial\PYGZus{}fit}\PYG{p}{(}\PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{classes}\PYG{o}{=}\PYG{n}{classes}\PYG{p}{)}
        \PYG{n}{value} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{score}\PYG{p}{(}\PYG{n}{x\PYGZus{}valid}\PYG{p}{,} \PYG{n}{y\PYGZus{}valid}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Report intermediate objective value.}
        \PYG{n}{trial}\PYG{o}{.}\PYG{n}{report}\PYG{p}{(}\PYG{n}{value}\PYG{p}{,} \PYG{n}{step}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Handle pruning based on the intermediate value.}
        \PYG{k}{if} \PYG{n}{trial}\PYG{o}{.}\PYG{n}{should\PYGZus{}prune}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{raise} \PYG{n}{optuna}\PYG{o}{.}\PYG{n}{TrialPruned}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{value}
\end{sphinxVerbatim}
}


\section{Running the Optimization}
\label{\detokenize{Optuna_VizWalkthrough:Running-the-Optimization}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{optuna}

\PYG{n}{optuna}\PYG{o}{.}\PYG{n}{logging}\PYG{o}{.}\PYG{n}{set\PYGZus{}verbosity}\PYG{p}{(}\PYG{n}{optuna}\PYG{o}{.}\PYG{n}{logging}\PYG{o}{.}\PYG{n}{WARNING}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} This verbosity change is just to simplify the notebook output.}

\PYG{n}{study} \PYG{o}{=} \PYG{n}{optuna}\PYG{o}{.}\PYG{n}{create\PYGZus{}study}\PYG{p}{(}\PYG{n}{direction}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{maximize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{pruner}\PYG{o}{=}\PYG{n}{optuna}\PYG{o}{.}\PYG{n}{pruners}\PYG{o}{.}\PYG{n}{MedianPruner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{study}\PYG{o}{.}\PYG{n}{optimize}\PYG{p}{(}\PYG{n}{objective}\PYG{p}{,} \PYG{n}{n\PYGZus{}trials}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Visualizing the Optimization History}
\label{\detokenize{Optuna_VizWalkthrough:Visualizing-the-Optimization-History}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{from} \PYG{n+nn}{optuna}\PYG{n+nn}{.}\PYG{n+nn}{visualization} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}optimization\PYGZus{}history}

\PYG{n}{plot\PYGZus{}optimization\PYGZus{}history}\PYG{p}{(}\PYG{n}{study}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Visualizing the Learning Curves of the Trials}
\label{\detokenize{Optuna_VizWalkthrough:Visualizing-the-Learning-Curves-of-the-Trials}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{from} \PYG{n+nn}{optuna}\PYG{n+nn}{.}\PYG{n+nn}{visualization} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}intermediate\PYGZus{}values}

\PYG{n}{plot\PYGZus{}intermediate\PYGZus{}values}\PYG{p}{(}\PYG{n}{study}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Visualizing High\sphinxhyphen{}dimensional Parameter Relationships}
\label{\detokenize{Optuna_VizWalkthrough:id1}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{from} \PYG{n+nn}{optuna}\PYG{n+nn}{.}\PYG{n+nn}{visualization} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}parallel\PYGZus{}coordinate}

\PYG{n}{plot\PYGZus{}parallel\PYGZus{}coordinate}\PYG{p}{(}\PYG{n}{study}\PYG{p}{)}
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{plot\PYGZus{}parallel\PYGZus{}coordinate}\PYG{p}{(}\PYG{n}{study}\PYG{p}{,} \PYG{n}{params}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lr\PYGZus{}init}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}units\PYGZus{}l0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Visualizing Parameter Relationships}
\label{\detokenize{Optuna_VizWalkthrough:Visualizing-Parameter-Relationships}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{from} \PYG{n+nn}{optuna}\PYG{n+nn}{.}\PYG{n+nn}{visualization} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}contour}

\PYG{n}{plot\PYGZus{}contour}\PYG{p}{(}\PYG{n}{study}\PYG{p}{)}
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{plot\PYGZus{}contour}\PYG{p}{(}\PYG{n}{study}\PYG{p}{,} \PYG{n}{params}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}units\PYGZus{}l0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}units\PYGZus{}l1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Visualizing Individual Parameters}
\label{\detokenize{Optuna_VizWalkthrough:Visualizing-Individual-Parameters}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{from} \PYG{n+nn}{optuna}\PYG{n+nn}{.}\PYG{n+nn}{visualization} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}slice}

\PYG{n}{plot\PYGZus{}slice}\PYG{p}{(}\PYG{n}{study}\PYG{p}{)}
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{plot\PYGZus{}slice}\PYG{p}{(}\PYG{n}{study}\PYG{p}{,} \PYG{n}{params}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}units\PYGZus{}l0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}units\PYGZus{}l1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Visualizing Parameter Importances}
\label{\detokenize{Optuna_VizWalkthrough:Visualizing-Parameter-Importances}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{from} \PYG{n+nn}{optuna}\PYG{n+nn}{.}\PYG{n+nn}{visualization} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}param\PYGZus{}importances}

\PYG{n}{plot\PYGZus{}param\PYGZus{}importances}\PYG{p}{(}\PYG{n}{study}\PYG{p}{)}
\end{sphinxVerbatim}
}


\chapter{STL and MTL GP Regression Walkthrough}
\label{\detokenize{Gaussian_Process_Walkthrough:STL-and-MTL-GP-Regression-Walkthrough}}\label{\detokenize{Gaussian_Process_Walkthrough::doc}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[3]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}from design import ModelTraining}
\PYG{k+kn}{from} \PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{SyntheticData} \PYG{k}{as} \PYG{n}{SD}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{time} \PYG{k+kn}{import} \PYG{n}{time}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{mtl}\PYG{n+nn}{.}\PYG{n+nn}{MTL\PYGZus{}GP} \PYG{k}{as} \PYG{n+nn}{MtlGP}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}

\end{sphinxVerbatim}
}


\section{Setting Up Datasets}
\label{\detokenize{Gaussian_Process_Walkthrough:Setting-Up-Datasets}}
The very first step to running through these Gaussian Process Tutorials is retrieving some data to train our models on. Here we are using the CTRP, GDSC and CCLE datasets mentioned in the introduction.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[4]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{importlib}
\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{MtlGP}\PYG{p}{)}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{SD}\PYG{o}{.}\PYG{n}{SyntheticDataCreator}\PYG{p}{(}\PYG{n}{num\PYGZus{}tasks}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{n}{cellsPerTask}\PYG{o}{=}\PYG{l+m+mi}{400}\PYG{p}{,} \PYG{n}{drugsPerTask}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{function}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cosine}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
             \PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{noise}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{graph}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{test\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{)}
\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{prepare\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Single Task Gaussian Process Example}
\label{\detokenize{Gaussian_Process_Walkthrough:Single-Task-Gaussian-Process-Example}}
below is an exaple of training and testing a basic Sparse Gaussian Process from gpytorch with our data.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{regressor}\PYG{n+nn}{.}\PYG{n+nn}{SparseGP} \PYG{k}{as} \PYG{n+nn}{SGP}
\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{SGP}\PYG{p}{)}

\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{n}{sparsegp} \PYG{o}{=} \PYG{n}{SGP}\PYG{o}{.}\PYG{n}{SparseGPRegression}\PYG{p}{(}\PYG{n}{num\PYGZus{}iters}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{length\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{noise\PYGZus{}covar}\PYG{o}{=}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{n}{n\PYGZus{}inducing\PYGZus{}points}\PYG{o}{=}\PYG{l+m+mi}{250}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{datasets}\PYG{p}{:}
    \PYG{n}{sparsegp}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{,}
               \PYG{n}{y}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{,}
               \PYG{n}{cat\PYGZus{}point}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{cat\PYGZus{}point}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]} \PYG{o}{=} \PYG{n}{sparsegp}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rmse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rmse}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}
\end{sphinxVerbatim}
}

Next, we have a more complex method, composite kernel Gaussian Process Regression

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{regressor}\PYG{n+nn}{.}\PYG{n+nn}{SparseGPCompositeKernel} \PYG{k}{as} \PYG{n+nn}{sgpc}
\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{sgpc}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{n}{sparsegpcomp} \PYG{o}{=} \PYG{n}{sgpc}\PYG{o}{.}\PYG{n}{SparseGPCompositeKernelRegression}\PYG{p}{(}\PYG{n}{num\PYGZus{}iters}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{length\PYGZus{}scale\PYGZus{}cell}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{length\PYGZus{}scale\PYGZus{}drug}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{noise\PYGZus{}covar}\PYG{o}{=}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{n}{n\PYGZus{}inducing\PYGZus{}points}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{datasets}\PYG{p}{:}
    \PYG{n}{sparsegpcomp}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{,}
               \PYG{n}{y}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{,}
               \PYG{n}{cat\PYGZus{}point}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{cat\PYGZus{}point}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]} \PYG{o}{=} \PYG{n}{sparsegpcomp}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rmse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rmse}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Multitask Background}
\label{\detokenize{Gaussian_Process_Walkthrough:Multitask-Background}}
Given a set of observations \(y_0\) we wish to learn parameters \(\theta_x\) and \(k^x\) of the matrix \(K_f\). \(k^x\) is a covariance function over the inputs and \(\theta_x\) are the parameters for that specific covariance function


\section{Hadamard Product MTL}
\label{\detokenize{Gaussian_Process_Walkthrough:Hadamard-Product-MTL}}
A clear limitation of the last method is that although it is technically multitask, it will fail to capture most task relationships. In order to do this I’ll introduce another spin on vanilla GP Regression.

Now we just have one model parameterized as: \begin{align*}
y_{i} &= f(x_i) + \varepsilon_{i} \\
f &\sim \mathcal{GP}(C_t,K_{\theta}) \\
\theta &\sim p(\theta) \\
\varepsilon_{i} &\stackrel{iid}{\sim} \mathcal{N}(0, \sigma^2)  \
\end{align*}

With one key difference. Our kernel is now defined as: \(K([x,i],[x',j]) = k_{inputs}(x,x') * k_{tasks}(i,j)\) where \$ k\_\{tasks\} \$ is an “index kernel”, essentially a lookup table for inter\sphinxhyphen{}task covariance. This lookup table is defined \(\forall \ i,j \in\) the set of tasks \(T\). Here’s a basic example with 4 datapoints and 2 tasks.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{MtlGP}\PYG{p}{)}

\PYG{n}{hadamardMTL} \PYG{o}{=} \PYG{n}{MtlGP}\PYG{o}{.}\PYG{n}{HadamardMTL}\PYG{p}{(}\PYG{n}{num\PYGZus{}iters}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{,} \PYG{n}{length\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{noise\PYGZus{}covar}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{24}\PYG{p}{,} \PYG{n}{n\PYGZus{}inducing\PYGZus{}points}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYGZbs{}
                                \PYG{n}{composite}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{07}\PYG{p}{,} \PYG{n}{validate}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}\PYG{n}{bias}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}\PYG{n}{stabilize}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}


\PYG{n}{hadamardMTL}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{y}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{catpt}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{cat\PYGZus{}point}\PYG{p}{)}


\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{hadamardMTL}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rmse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rmse}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Example Visualizing Covariance Using Getter}
\label{\detokenize{Gaussian_Process_Walkthrough:Example-Visualizing-Covariance-Using-Getter}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{full\PYGZus{}covar} \PYG{o}{=} \PYG{n}{hadamardMTL}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{getCovar}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{full\PYGZus{}covar}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{hadamardMTL}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{getCovar}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{from} \PYG{n+nn}{mpl\PYGZus{}toolkits}\PYG{n+nn}{.}\PYG{n+nn}{axes\PYGZus{}grid1} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}axes\PYGZus{}locatable}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{task\PYGZus{}covar} \PYG{o}{=} \PYG{n}{hadamardMTL}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{getTaskCovar}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} cast from torch to numpy}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{task\PYGZus{}covar}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Reds}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{200}\PYG{p}{,}\PYG{l+m+mi}{800}\PYG{p}{,}\PYG{l+m+mi}{1300}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{datasets}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{200}\PYG{p}{,}\PYG{l+m+mi}{800}\PYG{p}{,}\PYG{l+m+mi}{1300}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticklabels}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{datasets}\PYG{p}{)}
\PYG{n}{divider} \PYG{o}{=} \PYG{n}{make\PYGZus{}axes\PYGZus{}locatable}\PYG{p}{(}\PYG{n}{ax}\PYG{p}{)}
\PYG{n}{cax} \PYG{o}{=} \PYG{n}{divider}\PYG{o}{.}\PYG{n}{append\PYGZus{}axes}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{right}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{10}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{pad}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{cbar} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{cax} \PYG{o}{=} \PYG{n}{cax}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Full Multitask GP with Multitask Kernel}
\label{\detokenize{Gaussian_Process_Walkthrough:Full-Multitask-GP-with-Multitask-Kernel}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{MtlGP}\PYG{p}{)}

\PYG{n}{gpymtl} \PYG{o}{=} \PYG{n}{MtlGP}\PYG{o}{.}\PYG{n}{GPyFullMTL}\PYG{p}{(}\PYG{n}{num\PYGZus{}iters}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{,} \PYG{n}{length\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{n}{noise\PYGZus{}covar}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n\PYGZus{}inducing\PYGZus{}points}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,}  \PYG{n}{num\PYGZus{}tasks}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{05}\PYG{p}{)}


\PYG{n}{gpymtl}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{y}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{cat\PYGZus{}point}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{cat\PYGZus{}point}\PYG{p}{)}

\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[10]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{gpymtl}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rmse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{i} \PYG{o}{+}\PYG{o}{=}  \PYG{l+m+mi}{1}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rmse}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.5526002760370554 0
0.721262580126851 1
0.7105683397091712 2
\end{sphinxVerbatim}
}


\section{Example Find Initial Conditions}
\label{\detokenize{Gaussian_Process_Walkthrough:Example-Find-Initial-Conditions}}
In order to understand what parameters to start at, we can test different configurations of initial conditions

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{importlib}
\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{MtlGP}\PYG{p}{)}
\PYG{n}{multiBias} \PYG{o}{=} \PYG{n}{MtlGP}\PYG{o}{.}\PYG{n}{HadamardMTL}\PYG{p}{(}\PYG{n}{num\PYGZus{}iters}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{noise\PYGZus{}covar}\PYG{o}{=}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{n}{n\PYGZus{}inducing\PYGZus{}points}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{multitask\PYGZus{}kernel}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{}testing \PYGZsh{}0)}

\PYG{n}{multiBias}\PYG{o}{.}\PYG{n}{\PYGZus{}find\PYGZus{}initial\PYGZus{}conditions}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYGZbs{}
                                   \PYG{n}{n\PYGZus{}restarts}\PYG{o}{=}\PYG{l+m+mi}{800}\PYG{p}{,}\PYG{n}{n\PYGZus{}iters}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{n\PYGZus{}inducing\PYGZus{}points}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{)}
\end{sphinxVerbatim}
}

(tensor(1.2674, grad\_fn=), \{‘likelihood.noise\_covar.noise’: 0.7006388902664185, ‘covar\_module.lengthscale’: 10.444199562072754\})

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[2]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{sys}

\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k+kn}{from} \PYG{n+nn}{design} \PYG{k+kn}{import} \PYG{n}{ModelTraining}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{matrix\PYGZus{}factorization}\PYG{n+nn}{.}\PYG{n+nn}{FeaturizedNCF} \PYG{k}{as} \PYG{n+nn}{NCF\PYGZus{}feat}
\PYG{k+kn}{import} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{matrix\PYGZus{}factorization}\PYG{n+nn}{.}\PYG{n+nn}{CustomInputNCF} \PYG{k}{as} \PYG{n+nn}{NCF}
\PYG{k+kn}{from} \PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{SyntheticData} \PYG{k}{as} \PYG{n}{SD}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{time} \PYG{k+kn}{import} \PYG{n}{time}
\PYG{k+kn}{from} \PYG{n+nn}{UTILS} \PYG{k+kn}{import} \PYG{n}{utils}
\PYG{k+kn}{import} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{mtl}\PYG{n+nn}{.}\PYG{n+nn}{NCF\PYGZus{}MTL} \PYG{k}{as} \PYG{n+nn}{NCF\PYGZus{}MTL}

\end{sphinxVerbatim}
}

\# NonFeaturized NCF

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[3]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{c+c1}{\PYGZsh{}\PYGZpc{}\PYGZpc{}capture}
\PYG{k+kn}{import} \PYG{n+nn}{importlib}
\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{NCF}\PYG{p}{)}

\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{SD}\PYG{o}{.}\PYG{n}{SyntheticDataCreator}\PYG{p}{(}\PYG{n}{num\PYGZus{}tasks}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{n}{cellsPerTask}\PYG{o}{=}\PYG{l+m+mi}{400}\PYG{p}{,} \PYG{n}{drugsPerTask}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{function}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cosine}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
             \PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{noise}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{graph}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{test\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{)}
\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{prepare\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{hyperparams} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{batch\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{32}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epochs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{200}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[64,32,16,8]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{learner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rmsprop}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}factors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}neg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{4}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[0,0,0,0]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}mf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{verbose}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{warm\PYGZus{}start}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{k+kc}{False}\PYG{p}{\PYGZcb{}}

\PYG{n}{NCF1}  \PYG{o}{=} \PYG{n}{NCF}\PYG{o}{.}\PYG{n}{Neural\PYGZus{}Collaborative\PYGZus{}Filtering}\PYG{p}{(}\PYG{n}{hyperparams}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{non\PYGZus{}feature\PYGZus{}based}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} can be feature based}
        \PYG{c+c1}{\PYGZsh{} it needs to be non bc models does feature transform}

\PYG{c+c1}{\PYGZsh{} iterate through datasets in single task learning paradigm}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{datasets}\PYG{p}{:}
    \PYG{n}{NCF1}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{trainRatings}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]} \PYG{o}{=} \PYG{n}{NCF1}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{testRatings}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}rmse}
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rmse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rmse}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
MODEL HAS BEEN DEFINED
TRAINING{\ldots}
NCF reinitialized
0
PREDICTING{\ldots}
TRAINING{\ldots}
NCF reinitialized
0
75
150
PREDICTING{\ldots}
TRAINING{\ldots}
NCF reinitialized
0
75
150
PREDICTING{\ldots}
0.8048341314316828 0
0.9302123806595449 1
0.9556996063227641 2
\end{sphinxVerbatim}
}


\chapter{Featurized NCF Example}
\label{\detokenize{NCF_Walkthrough:Featurized-NCF-Example}}\label{\detokenize{NCF_Walkthrough::doc}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[4]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{c+c1}{\PYGZsh{}\PYGZpc{}\PYGZpc{}capture}
\PYG{k+kn}{import} \PYG{n+nn}{importlib}
\PYG{c+c1}{\PYGZsh{}reload python import so we don\PYGZsq{}t have to start and restart kernel}
\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{NCF\PYGZus{}feat}\PYG{p}{)}




\PYG{n}{hyperparams} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{batch\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epochs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[64,32,16,8]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{learner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adam}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.001}\PYG{p}{,} \PYGZbs{}
               \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}factors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}neg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[0.01,0,0,0.01]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}mf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.01}\PYG{p}{,}\PYGZbs{}
               \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{verbose}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{warm\PYGZus{}start}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{k+kc}{False}\PYG{p}{\PYGZcb{}}
\PYG{n}{NCF2}  \PYG{o}{=} \PYG{n}{NCF\PYGZus{}feat}\PYG{o}{.}\PYG{n}{Neural\PYGZus{}Collaborative\PYGZus{}Filtering\PYGZus{}Features}\PYG{p}{(}\PYG{n}{hyperparams}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{feature\PYGZus{}based}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} can be feature based}
        \PYG{c+c1}{\PYGZsh{} it needs to be non bc models does feature transform}

\PYG{c+c1}{\PYGZsh{} iterate through datasets in single task learning paradigm}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{datasets}\PYG{p}{:}
    \PYG{n}{NCF2}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]} \PYG{o}{=} \PYG{n}{NCF2}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}rmse}
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rmse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rmse}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
model has been defined
TRAINING{\ldots}
NCF reinitialized
break{\ldots} model converged
PREDICTING{\ldots}
(1200, 20)
TRAINING{\ldots}
NCF reinitialized
break{\ldots} model converged
PREDICTING{\ldots}
(1200, 20)
TRAINING{\ldots}
NCF reinitialized
break{\ldots} model converged
PREDICTING{\ldots}
(1200, 20)
0.25058302922958736 0
0.24961774086704824 1
0.25494508025729135 2
\end{sphinxVerbatim}
}


\chapter{MTL NCF with Pooled MLP Example}
\label{\detokenize{NCF_Walkthrough:MTL-NCF-with-Pooled-MLP-Example}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[5]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{c+c1}{\PYGZsh{}\PYGZpc{}\PYGZpc{}capture}
\PYG{k+kn}{import} \PYG{n+nn}{importlib}
\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{NCF\PYGZus{}MTL}\PYG{p}{)}


\PYG{n}{hyperparams\PYGZus{}mtlmlp} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{batch\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epochs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{150}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[64,32,16,8]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{learner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adam}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{.}\PYG{l+m+mi}{001}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mlp\PYGZus{}lr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{.}\PYG{l+m+mi}{001}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}factors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[0,0,0,.01]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}mf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{verbose}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}


\PYG{n}{NCF3} \PYG{o}{=} \PYG{n}{NCF\PYGZus{}MTL}\PYG{o}{.}\PYG{n}{Neural\PYGZus{}Collaborative\PYGZus{}Filtering\PYGZus{}FeaturesMTLMLP}\PYG{p}{(}\PYG{n}{hyperparams\PYGZus{}mtlmlp}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{feature\PYGZus{}based}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{NCF3}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{y}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{cat\PYGZus{}point}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{cat\PYGZus{}point}\PYG{p}{)}

\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{NCF3}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}rmse}
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rmse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rmse}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}

\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
model has been defined
PREDICTING{\ldots}
0.8660533164359661 0
0.6996997536437438 1
0.624245763614103 2
\end{sphinxVerbatim}
}


\chapter{MTL NCF with Pooled MF}
\label{\detokenize{NCF_Walkthrough:MTL-NCF-with-Pooled-MF}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[6]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{c+c1}{\PYGZsh{}\PYGZpc{}\PYGZpc{}capture}
\PYG{k+kn}{import} \PYG{n+nn}{importlib}
\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{NCF\PYGZus{}MTL}\PYG{p}{)}

\PYG{n}{hyperparams\PYGZus{}mtlmlp} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{batch\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epochs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{150}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[64,32,16,8]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{learner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adam}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{.}\PYG{l+m+mi}{001}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mlp\PYGZus{}lr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{.}\PYG{l+m+mi}{001}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}factors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[0,0,0,.01]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}mf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{verbose}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}


\PYG{n}{NCF3} \PYG{o}{=} \PYG{n}{NCF\PYGZus{}MTL}\PYG{o}{.}\PYG{n}{Neural\PYGZus{}Collaborative\PYGZus{}Filtering\PYGZus{}FeaturesMTLMLP}\PYG{p}{(}\PYG{n}{hyperparams\PYGZus{}mtlmlp}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{feature\PYGZus{}based}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{NCF3}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{y}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{cat\PYGZus{}point}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{cat\PYGZus{}point}\PYG{p}{)}

\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{NCF3}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}rmse}
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rmse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rmse}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}

\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
model has been defined
PREDICTING{\ldots}
0.7123474321731185 0
1.7743642422838766 1
0.4966135018616864 2
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[32]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{c+c1}{\PYGZsh{}\PYGZpc{}\PYGZpc{}capture}
\PYG{k+kn}{import} \PYG{n+nn}{importlib}
\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{NCF\PYGZus{}MTL}\PYG{p}{)}

\PYG{n}{hyperparams\PYGZus{}mtlmf} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{batch\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epochs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{150}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[64,32,16,8]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{learner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adam}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{.}\PYG{l+m+mi}{001}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mf\PYGZus{}lr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{.}\PYG{l+m+mi}{001}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}factors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[0,0,0,.01]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}mf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{verbose}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}


\PYG{n}{NCF4} \PYG{o}{=} \PYG{n}{NCF\PYGZus{}MTL}\PYG{o}{.}\PYG{n}{Neural\PYGZus{}Collaborative\PYGZus{}Filtering\PYGZus{}FeaturesMTLMF}\PYG{p}{(}\PYG{n}{hyperparams\PYGZus{}mtlmf}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{feature\PYGZus{}based}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{NCF4}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{y}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{cat\PYGZus{}point}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{cat\PYGZus{}point}\PYG{p}{)}

\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{NCF4}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}rmse}
\PYG{k}{for} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n}{y\PYGZus{}pred}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rmse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rmse}\PYG{p}{,} \PYG{n}{name}\PYG{p}{)}

\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
PREDICTING{\ldots}
0.6981196213543954 CCLE
0.5048938257444706 GDSC
0.7784718127828185 CTRP
\end{sphinxVerbatim}
}


\chapter{Featurized NCF Train Test Curve Example}
\label{\detokenize{NCF_Walkthrough:Featurized-NCF-Train-Test-Curve-Example}}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[7]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{c+c1}{\PYGZsh{}\PYGZpc{}\PYGZpc{}capture}
\PYG{k+kn}{import} \PYG{n+nn}{importlib}
\PYG{c+c1}{\PYGZsh{}reload python import so we don\PYGZsq{}t have to start and restart kernel}
\PYG{n}{importlib}\PYG{o}{.}\PYG{n}{reload}\PYG{p}{(}\PYG{n}{NCF\PYGZus{}feat}\PYG{p}{)}

\PYG{n}{hyperparams} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{batch\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epochs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[64,32,16,8]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{learner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adam}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mf\PYGZus{}pretrain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mlp\PYGZus{}pretrain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}factors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}neg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{out}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{path}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data/}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[0.01,0,0,0.01]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reg\PYGZus{}mf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{verbose}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{warm\PYGZus{}start}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{k+kc}{False}\PYG{p}{\PYGZcb{}}
\PYG{n}{model}  \PYG{o}{=} \PYG{n}{NCF\PYGZus{}feat}\PYG{o}{.}\PYG{n}{Neural\PYGZus{}Collaborative\PYGZus{}Filtering\PYGZus{}Features}\PYG{p}{(}\PYG{n}{hyperparams}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{feature\PYGZus{}based}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} can be feature based}
        \PYG{c+c1}{\PYGZsh{} it needs to be non bc models does feature transform}
\PYG{n}{epochs} \PYG{o}{=} \PYG{l+m+mi}{600}
\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{64}
\PYG{n}{plot\PYGZus{}counter} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{datasets}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{k}\PYG{p}{)}
    \PYG{n}{train\PYGZus{}rmses} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{test\PYGZus{}rmses} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

    \PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{epochs}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{epoch} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{10} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{epoch : }\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{,} \PYG{n}{epoch}\PYG{p}{)}
        \PYG{n}{t1} \PYG{o}{=} \PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Generate training instances}
        \PYG{n}{train\PYGZus{}x} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{30000}\PYG{p}{]}
        \PYG{n}{train\PYGZus{}y} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{30000}\PYG{p}{]}
        \PYG{n}{test\PYGZus{}x} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10000}\PYG{p}{]}
        \PYG{n}{test\PYGZus{}y} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10000}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{}        overlap = 0}
\PYG{c+c1}{\PYGZsh{}         for y in test\PYGZus{}y:}
\PYG{c+c1}{\PYGZsh{}             if y in train\PYGZus{}y:}
\PYG{c+c1}{\PYGZsh{}                 overlap += 1}
\PYG{c+c1}{\PYGZsh{}         print(overlap, \PYGZdq{}Overlap\PYGZdq{})}

        \PYG{n}{train\PYGZus{}hist} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}inputs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{train\PYGZus{}x}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item\PYGZus{}inputs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{train\PYGZus{}x}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}} \PYGZbs{}
                                     \PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{train\PYGZus{}y}\PYG{p}{)}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{verbose}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{shuffle}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
        \PYG{n}{test\PYGZus{}hist} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{evaluate}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}inputs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{test\PYGZus{}x}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item\PYGZus{}inputs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{test\PYGZus{}x}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{test\PYGZus{}y}\PYG{p}{)}\PYG{p}{,}
                            \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{verbose}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{return\PYGZus{}dict}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{n}{t2} \PYG{o}{=} \PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{}print(\PYGZdq{}train: \PYGZdq{},train\PYGZus{}hist.history[\PYGZsq{}root\PYGZus{}mean\PYGZus{}squared\PYGZus{}error\PYGZsq{}], \PYGZdq{}test: \PYGZdq{}, test\PYGZus{}hist[\PYGZsq{}root\PYGZus{}mean\PYGZus{}squared\PYGZus{}error\PYGZsq{}])}

        \PYG{n}{train\PYGZus{}rmses}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{train\PYGZus{}hist}\PYG{o}{.}\PYG{n}{history}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{root\PYGZus{}mean\PYGZus{}squared\PYGZus{}error}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{test\PYGZus{}rmses}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{test\PYGZus{}hist}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{root\PYGZus{}mean\PYGZus{}squared\PYGZus{}error}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{epoch} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{10} \PYG{o+ow}{and} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{train\PYGZus{}rmses}\PYG{p}{[}\PYG{n}{epoch}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}\PYG{p}{:}\PYG{n}{epoch}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{n}{train\PYGZus{}rmses}\PYG{p}{[}\PYG{n}{epoch}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}\PYG{p}{:}\PYG{n}{epoch}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{o}{.}\PYG{l+m+mi}{008}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BREAK}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
            \PYG{k}{break}
    \PYG{n}{axs}\PYG{p}{[}\PYG{n}{plot\PYGZus{}counter}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{train\PYGZus{}rmses}\PYG{p}{)}
    \PYG{n}{axs}\PYG{p}{[}\PYG{n}{plot\PYGZus{}counter}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{test\PYGZus{}rmses}\PYG{p}{)}
    \PYG{n}{axs}\PYG{p}{[}\PYG{n}{plot\PYGZus{}counter}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{n}{k}\PYG{p}{)}
    \PYG{n}{axs}\PYG{p}{[}\PYG{n}{plot\PYGZus{}counter}\PYG{p}{]}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{validation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{axs}\PYG{p}{[}\PYG{n}{plot\PYGZus{}counter}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RMSE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{axs}\PYG{p}{[}\PYG{n}{plot\PYGZus{}counter}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{EPOCH}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plot\PYGZus{}counter} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{min train err: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n+nb}{min}\PYG{p}{(}\PYG{n}{train\PYGZus{}rmses}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{min test err: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{min}\PYG{p}{(}\PYG{n}{test\PYGZus{}rmses}\PYG{p}{)} \PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
model has been defined
0
epoch :  0
epoch :  10
epoch :  20
BREAK
min train err:  [0.2359551042318344] min test err:  0.23274201154708862
1
epoch :  0
epoch :  10
BREAK
min train err:  [0.22925961017608643] min test err:  0.22942525148391724
2
epoch :  0
epoch :  10
BREAK
min train err:  [0.23665441572666168] min test err:  0.2217150777578354
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{nbsphinx-stderr}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
/usr/tce/packages/python/python-3.7.2/lib/python3.7/site-packages/matplotlib/figure.py:2366: UserWarning: This figure includes Axes that are not compatible with tight\_layout, so results might be incorrect.
  warnings.warn("This figure includes Axes that are not compatible "
\end{sphinxVerbatim}
}

\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\noindent\sphinxincludegraphics[width=566\sphinxpxdimen,height=1430\sphinxpxdimen]{{NCF_Walkthrough_11_2}.png}

\end{nbsphinxfancyoutput}


\chapter{Python Script Example}
\label{\detokenize{exp:python-script-example}}\label{\detokenize{exp::doc}}

\section{Python Script Example}
\label{\detokenize{exp:id1}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k+kn}{from} \PYG{n+nn}{design} \PYG{k+kn}{import} \PYG{n}{ModelTraining}
\PYG{k+kn}{from} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{mtl}\PYG{n+nn}{.}\PYG{n+nn}{MF\PYGZus{}MTL} \PYG{k+kn}{import} \PYG{n}{MF\PYGZus{}MTL}
\PYG{k+kn}{from} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{matrix\PYGZus{}factorization}\PYG{n+nn}{.}\PYG{n+nn}{MF\PYGZus{}STL} \PYG{k+kn}{import} \PYG{n}{MF\PYGZus{}STL}

\PYG{c+c1}{\PYGZsh{} from methods.regressor.FFNN import FeedForwardNN}
\PYG{k+kn}{from} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{matrix\PYGZus{}factorization}\PYG{n+nn}{.}\PYG{n+nn}{MF} \PYG{k+kn}{import} \PYG{n}{SVD\PYGZus{}MF}\PYG{p}{,} \PYG{n}{NonNegative\PYGZus{}MF}
\PYG{k+kn}{from} \PYG{n+nn}{methods}\PYG{n+nn}{.}\PYG{n+nn}{knn}\PYG{n+nn}{.}\PYG{n+nn}{KNN} \PYG{k+kn}{import} \PYG{n}{KNN\PYGZus{}Normalized}
\PYG{k+kn}{from} \PYG{n+nn}{datasets}\PYG{n+nn}{.}\PYG{n+nn}{DrugCellLines} \PYG{k+kn}{import} \PYG{n}{DrugCellLinesMTL}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}

    \PYG{n}{drug\PYGZus{}transform} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pca}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}comp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}
    \PYG{n}{cell\PYGZus{}transform} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pca}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}comp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}
    \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{DrugCellLinesMTL}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CCLE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GDSC}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CTRP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NCI60}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{common}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
                               \PYG{n}{unseen\PYGZus{}cells}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
                               \PYG{n}{test\PYGZus{}split}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{drug\PYGZus{}transform}\PYG{o}{=}\PYG{n}{drug\PYGZus{}transform}\PYG{p}{,}
                               \PYG{n}{cell\PYGZus{}transform}\PYG{o}{=}\PYG{n}{cell\PYGZus{}transform}\PYG{p}{)}
    \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{prepare\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}



    \PYG{n}{methods} \PYG{o}{=} \PYG{p}{[}\PYG{n}{SVD\PYGZus{}MF}\PYG{p}{(}\PYG{n}{n\PYGZus{}factors}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{,}
               \PYG{n}{KNN\PYGZus{}Normalized}\PYG{p}{(}\PYG{n}{k}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{]}

    \PYG{n}{metrics} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rmse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{explained\PYGZus{}variance\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mae}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

    \PYG{n}{exp\PYGZus{}folder} \PYG{o}{=} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}file\PYGZus{}\PYGZus{}}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.py}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{exp} \PYG{o}{=} \PYG{n}{ModelTraining}\PYG{p}{(}\PYG{n}{exp\PYGZus{}folder}\PYG{p}{)}
    \PYG{n}{exp}\PYG{o}{.}\PYG{n}{execute}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{,} \PYG{n}{methods}\PYG{p}{,} \PYG{n}{metrics}\PYG{p}{,} \PYG{n}{nruns}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{exp}\PYG{o}{.}\PYG{n}{generate\PYGZus{}report}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}




\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}

\begin{sphinxthebibliography}{GPB+18}
\bibitem[eal19]{zcite:ccle}
Ghandi et al. Next\sphinxhyphen{}generation characterization of the cancer cell line encyclopedia. \sphinxstyleemphasis{Nature}, 569:1–6, 05 2019. \sphinxhref{https://doi.org/10.1038/s41586-019-1186-3}{doi:10.1038/s41586\sphinxhyphen{}019\sphinxhyphen{}1186\sphinxhyphen{}3}%
\begin{footnote}[8]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1038/s41586-019-1186-3}
%
\end{footnote}.
\bibitem[GPB+18]{zcite:gardner2018gpytorch}
Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson. Gpytorch: blackbox matrix\sphinxhyphen{}matrix gaussian process inference with gpu acceleration. In \sphinxstyleemphasis{Advances in Neural Information Processing Systems}. 2018.
\bibitem[HLZ+17]{zcite:ncf}
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat\sphinxhyphen{}Seng Chua. Neural collaborative filtering. In \sphinxstyleemphasis{Proceedings of the 26th International Conference on World Wide Web}, WWW ‘17, 173–182. Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee. URL: \sphinxurl{https://doi.org/10.1145/3038912.3052569}, \sphinxhref{https://doi.org/10.1145/3038912.3052569}{doi:10.1145/3038912.3052569}%
\begin{footnote}[9]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1145/3038912.3052569}
%
\end{footnote}.
\bibitem[Hug20]{zcite:hug2020}
Nicolas Hug. Surprise: a python library for recommender systems. \sphinxstyleemphasis{Journal of Open Source Software}, 5(52):2174, 2020. URL: \sphinxurl{https://doi.org/10.21105/joss.02174}, \sphinxhref{https://doi.org/10.21105/joss.02174}{doi:10.21105/joss.02174}%
\begin{footnote}[10]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.21105/joss.02174}
%
\end{footnote}.
\bibitem[PGM+19]{zcite:neurips2019-9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: an imperative style, high\sphinxhyphen{}performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\textbackslash{}textquotesingle  Alché\sphinxhyphen{}Buc, E. Fox, and R. Garnett, editors, \sphinxstyleemphasis{Advances in Neural Information Processing Systems 32}, pages 8024–8035. Curran Associates, Inc., 2019. URL: \sphinxurl{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}.
\bibitem[Sch11]{zcite:nci}
Manfred Schwab, editor. \sphinxstyleemphasis{NCI 60 Cell Line Screen}, pages 2468–2468. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. URL: \sphinxurl{https://doi.org/10.1007/978-3-642-16483-5\_3987}, \sphinxhref{https://doi.org/10.1007/978-3-642-16483-5\_3987}{doi:10.1007/978\sphinxhyphen{}3\sphinxhyphen{}642\sphinxhyphen{}16483\sphinxhyphen{}5\_3987}%
\begin{footnote}[11]\sphinxAtStartFootnote
\sphinxnolinkurl{https://doi.org/10.1007/978-3-642-16483-5\_3987}
%
\end{footnote}.
\end{sphinxthebibliography}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{d}
\item\relax\sphinxstyleindexentry{datasets.SyntheticData}\sphinxstyleindexpageref{data:\detokenize{module-datasets.SyntheticData}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\footnotesize\raggedright\printindex
\end{document}